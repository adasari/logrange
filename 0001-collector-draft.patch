From 8cc4104170201f3e275f96de90f1e6af4630a4ab Mon Sep 17 00:00:00 2001
From: Dmitry Spasibenko <dspasibenko@gmail.com>
Date: Wed, 10 Oct 2018 14:29:40 -0700
Subject: [PATCH] collector draft

---
 cmd/collector/config.json                       |  31 ++
 cmd/collector/main.go                           | 144 ++++++
 cmd/collector/mixed-config.json                 |  47 ++
 pkg/collector/collector.go                      |  64 +++
 pkg/collector/ingestor/config.go                |  87 ++++
 pkg/collector/ingestor/encoder.go               |  60 +++
 pkg/collector/ingestor/ingestor.go              | 329 ++++++++++++
 pkg/collector/model/event.go                    |  52 ++
 pkg/collector/model/record.go                   |  46 ++
 pkg/collector/scanner/dtparser/dtparser.go      | 251 +++++++++
 pkg/collector/scanner/dtparser/dtparser_test.go |  13 +
 pkg/collector/scanner/json_parser.go            |  97 ++++
 pkg/collector/scanner/json_parser_test.go       |  71 +++
 pkg/collector/scanner/line_reader.go            |  87 ++++
 pkg/collector/scanner/lines_parser.go           |  79 +++
 pkg/collector/scanner/parser.go                 |  41 ++
 pkg/collector/scanner/scanner.go                | 652 ++++++++++++++++++++++++
 pkg/collector/scanner/scanner_test.go           | 172 +++++++
 pkg/collector/scanner/status_storage.go         |  59 +++
 pkg/collector/scanner/string_parser.go          | 158 ++++++
 pkg/collector/scanner/string_parser_test.go     |  61 +++
 pkg/collector/scanner/worker.go                 | 153 ++++++
 pkg/collector/status_updater.go                 | 161 ++++++
 pkg/{collection => dstruct}/lru.go              |   2 +-
 pkg/{collection => dstruct}/lru_test.go         |   2 +-
 pkg/dstruct/ring_buffer.go                      | 127 +++++
 pkg/dstruct/ring_buffer_test.go                 | 107 ++++
 pkg/dstruct/timeseries.go                       | 148 ++++++
 pkg/dstruct/timeseries_test.go                  |  99 ++++
 pkg/logevent/log_event.go                       | 126 +++++
 pkg/logevent/log_event_test.go                  |  80 +++
 pkg/logevent/tags.go                            | 143 ++++++
 pkg/logevent/tags_test.go                       |  67 +++
 pkg/logevent/transform.go                       | 195 +++++++
 pkg/logevent/transform_test.go                  | 170 ++++++
 pkg/proto/atmosphere/server.go                  |  10 +-
 pkg/util/byte.go                                |  11 +
 pkg/util/byte_test.go                           |  15 +
 pkg/util/file.go                                |  37 ++
 pkg/util/file_test.go                           |  42 ++
 pkg/util/format.go                              |  30 ++
 pkg/util/format_test.go                         |   1 +
 pkg/util/hash.go                                |  21 +
 pkg/util/hash/hash.go                           | 130 -----
 pkg/util/hash/hash_test.go                      |  37 --
 pkg/util/hash_test.go                           |   1 +
 pkg/util/json.go                                |  21 +
 pkg/util/json_test.go                           |   1 +
 pkg/util/net.go                                 |  50 ++
 pkg/util/net_test.go                            |  17 +
 pkg/util/secret.go                              |  17 +
 pkg/util/secret_test.go                         |  23 +
 pkg/util/string.go                              |  81 +++
 pkg/util/string_test.go                         |  14 +
 54 files changed, 4566 insertions(+), 174 deletions(-)
 create mode 100644 cmd/collector/config.json
 create mode 100644 cmd/collector/main.go
 create mode 100644 cmd/collector/mixed-config.json
 create mode 100644 pkg/collector/collector.go
 create mode 100644 pkg/collector/ingestor/config.go
 create mode 100644 pkg/collector/ingestor/encoder.go
 create mode 100644 pkg/collector/ingestor/ingestor.go
 create mode 100644 pkg/collector/model/event.go
 create mode 100644 pkg/collector/model/record.go
 create mode 100644 pkg/collector/scanner/dtparser/dtparser.go
 create mode 100644 pkg/collector/scanner/dtparser/dtparser_test.go
 create mode 100644 pkg/collector/scanner/json_parser.go
 create mode 100644 pkg/collector/scanner/json_parser_test.go
 create mode 100644 pkg/collector/scanner/line_reader.go
 create mode 100644 pkg/collector/scanner/lines_parser.go
 create mode 100644 pkg/collector/scanner/parser.go
 create mode 100644 pkg/collector/scanner/scanner.go
 create mode 100644 pkg/collector/scanner/scanner_test.go
 create mode 100644 pkg/collector/scanner/status_storage.go
 create mode 100644 pkg/collector/scanner/string_parser.go
 create mode 100644 pkg/collector/scanner/string_parser_test.go
 create mode 100644 pkg/collector/scanner/worker.go
 create mode 100644 pkg/collector/status_updater.go
 rename pkg/{collection => dstruct}/lru.go (99%)
 rename pkg/{collection => dstruct}/lru_test.go (99%)
 create mode 100644 pkg/dstruct/ring_buffer.go
 create mode 100644 pkg/dstruct/ring_buffer_test.go
 create mode 100644 pkg/dstruct/timeseries.go
 create mode 100644 pkg/dstruct/timeseries_test.go
 create mode 100644 pkg/logevent/log_event.go
 create mode 100644 pkg/logevent/log_event_test.go
 create mode 100644 pkg/logevent/tags.go
 create mode 100644 pkg/logevent/tags_test.go
 create mode 100644 pkg/logevent/transform.go
 create mode 100644 pkg/logevent/transform_test.go
 create mode 100644 pkg/util/byte.go
 create mode 100644 pkg/util/byte_test.go
 create mode 100644 pkg/util/file.go
 create mode 100644 pkg/util/file_test.go
 create mode 100644 pkg/util/format.go
 create mode 100644 pkg/util/format_test.go
 create mode 100644 pkg/util/hash.go
 delete mode 100644 pkg/util/hash/hash.go
 delete mode 100644 pkg/util/hash/hash_test.go
 create mode 100644 pkg/util/hash_test.go
 create mode 100644 pkg/util/json.go
 create mode 100644 pkg/util/json_test.go
 create mode 100644 pkg/util/net.go
 create mode 100644 pkg/util/net_test.go
 create mode 100644 pkg/util/secret.go
 create mode 100644 pkg/util/secret_test.go
 create mode 100644 pkg/util/string.go
 create mode 100644 pkg/util/string_test.go

diff --git a/cmd/collector/config.json b/cmd/collector/config.json
new file mode 100644
index 0000000..04c7391
--- /dev/null
+++ b/cmd/collector/config.json
@@ -0,0 +1,31 @@
+{
+  "ingestor": {
+    "server": "127.0.0.1:9966",
+	"retrySec": 5,
+    "heartBeatMs": 15000,
+    "packetMaxRecords": 1000,
+    "accessKey": "",
+    "secretKey": "",
+    "schemas": [{
+      "pathMatcher":"/*(?:.+/)*(?P<file>.+\\..+)",
+      "sourceId":"{file}",
+      "tags": {
+        "file": "{file}"
+      }
+    }]
+  },
+  "scanner": {
+    "scanPaths": ["/var/log/*.log", "/var/log/*/*.log"],
+    "scanPathsIntervalSec": 60,
+    "stateFlushIntervalSec": 10,
+    "eventMaxRecords": 1000,
+    "eventSendIntervalMs": 200,
+    "recordMaxSizeBytes": 16384,
+    "fileFormats": [{
+      "pathMatcher": ".*",
+      "timeFormats": ["DD/MMM/YYYY:HH:mm:ss ZZZZ"]
+    }]
+  },
+  "statusFile": "/opt/logrange/collector/status",
+  "stateFile": "/opt/logrange/collector/state"
+}
diff --git a/cmd/collector/main.go b/cmd/collector/main.go
new file mode 100644
index 0000000..aae89ad
--- /dev/null
+++ b/cmd/collector/main.go
@@ -0,0 +1,144 @@
+package main
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+	"github.com/logrange/logrange/pkg/collector"
+	"github.com/logrange/logrange/pkg/collector/ingestor"
+	"github.com/logrange/logrange/pkg/collector/scanner"
+	"os"
+	"os/signal"
+	"syscall"
+
+	"github.com/ghodss/yaml"
+	"github.com/jrivets/log4g"
+	"gopkg.in/alecthomas/kingpin.v2"
+)
+
+type (
+	args struct {
+		config      string
+		debug       bool
+		printStatus bool
+		printConfig string
+	}
+)
+
+const (
+	Version            = "0.0.1"
+	cDefaultConfigPath = "/opt/logrange/collector/config.json"
+)
+
+var (
+	logger = log4g.GetLogger("collector")
+)
+
+func main() {
+	defer log4g.Shutdown()
+	args := parseCLP()
+	if args.debug {
+		log4g.SetLogLevel("", log4g.TRACE)
+	}
+
+	cfg := collector.NewDefaultConfig()
+	err := cfg.LoadFromFile(args.config)
+	logger.Info("Loading config from ", args.config)
+	if err != nil {
+		if !os.IsNotExist(err) {
+			logger.Error("Unable to load config file=", args.config, "; cause err=", err)
+			os.Exit(1)
+		}
+
+		logger.Warn("Unable to load config file=", args.config, "; The file is not found")
+		cfg = collector.NewDefaultConfig()
+	}
+
+	if args.printConfig != "" {
+		var bCfg []byte
+		switch args.printConfig {
+		case "json":
+			bCfg, err = json.MarshalIndent(cfg, "", "    ")
+		case "yaml":
+			bCfg, err = yaml.Marshal(cfg)
+		default:
+			logger.Error("Unsupported config format ", args.printConfig, " 'json' or 'yaml' can be used.")
+			os.Exit(1)
+		}
+
+		if err != nil {
+			logger.Error("Internal error, could not form config in YAML format: ", err)
+			os.Exit(1)
+		}
+		fmt.Println("")
+		fmt.Println(string(bCfg))
+		os.Exit(0)
+	}
+
+	if args.printStatus {
+		su := collector.NewStatusFileUpdater(cfg, nil, nil)
+		su.PrintStatusFile()
+		os.Exit(0)
+	}
+
+	ctx, cancel := context.WithCancel(context.Background())
+	go func() {
+		sigChan := make(chan os.Signal, 1)
+		signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
+		select {
+		case s := <-sigChan:
+			logger.Warn("Handling signal=", s)
+			cancel()
+		}
+	}()
+
+	var (
+		scn *scanner.Scanner
+		ing *ingestor.Ingestor
+	)
+
+	scn, err = newScanner(cfg)
+	if err != nil {
+		logger.Fatal("Unable to create scanner; cause: ", err)
+		return
+	}
+	defer scn.Stop()
+
+	ing, err = ingestor.NewIngestor(cfg.Ingestor, ctx)
+	if err != nil {
+		logger.Fatal("Unable to create ingestor; cause: ", err)
+		return
+	}
+
+	sfu := collector.NewStatusFileUpdater(cfg, scn, ing)
+	sfu.Run(ctx)
+
+	done := ing.Run(ctx, scn.Events())
+	<-done
+}
+
+func parseCLP() *args {
+	var (
+		config      = kingpin.Flag("config-file", "Configuration file name").Default(cDefaultConfigPath).String()
+		debug       = kingpin.Flag("debug", "Enable debug log level").Bool()
+		printConfig = kingpin.Flag("print-config", "Print existing config").PlaceHolder("(json|yaml)").String()
+		status      = kingpin.Flag("print-status", "Print agent status, if it is running").Bool()
+	)
+	kingpin.Version(Version)
+	kingpin.Parse()
+
+	res := new(args)
+	res.config = *config
+	res.debug = *debug
+	res.printStatus = *status
+	res.printConfig = *printConfig
+	return res
+}
+
+func newScanner(cfg *collector.Config) (*scanner.Scanner, error) {
+	gsr, err := scanner.NewScanner(cfg.Scanner, scanner.NewFileStateStorage(cfg.StateFile))
+	if err == nil {
+		err = gsr.Start()
+	}
+	return gsr, err
+}
diff --git a/cmd/collector/mixed-config.json b/cmd/collector/mixed-config.json
new file mode 100644
index 0000000..a6efdb2
--- /dev/null
+++ b/cmd/collector/mixed-config.json
@@ -0,0 +1,47 @@
+{
+  "ingestor": {
+    "server": "127.0.0.1:9966",
+    "heartBeatMs": 15000,
+    "packetMaxRecords": 1000,
+    "accessKey": "",
+    "secretKey": "",
+    "schemas": [
+	{
+      "pathMatcher":"/*(?:.+/)*(?P<pod>.+)_(?P<ns>.+)_(?P<cname>.+)-(?P<cid>.+)\\..+",
+      "sourceId":"{cname}",
+      "tags": {
+        "pod": "{pod}",
+		"ns": "{ns}",
+		"cname": "{cname}",
+		"cid": "{cid}"
+      }
+    },
+	{
+      "pathMatcher":"/*(?:.+/)*(?P<file>.+\\..+)",
+      "sourceId":"{file}",
+      "tags": {
+        "file": "{file}"
+      }
+    }
+	]
+  },
+  "scanner": {
+    "scanPaths": ["/var/log/*.log", "/var/log/*/*.log"],
+    "scanPathsIntervalSec": 60,
+    "stateFile": "/tmp/collector.state",
+    "stateFlushIntervalSec": 10,
+    "eventMaxRecords": 1000,
+    "eventSendIntervalMs": 200,
+    "recordMaxSizeBytes": 16384,
+    "fileFormats": [
+	{
+      "pathMatcher": "/*(?:.+/)*(?P<pod>.+)_(?P<ns>.+)_(?P<cname>.+)-(?P<cid>.+)\\..+",
+      "format": "json"
+    },
+	{
+      "pathMatcher": ".*",
+      "timeFormats": ["DD/MMM/YYYY:HH:mm:ss ZZZZ"]
+    }
+	]
+  }
+}
diff --git a/pkg/collector/collector.go b/pkg/collector/collector.go
new file mode 100644
index 0000000..ae4cab3
--- /dev/null
+++ b/pkg/collector/collector.go
@@ -0,0 +1,64 @@
+package collector
+
+import (
+	"encoding/json"
+	"github.com/logrange/logrange/pkg/collector/ingestor"
+	"github.com/logrange/logrange/pkg/collector/scanner"
+	"gopkg.in/yaml.v2"
+	"io/ioutil"
+	"strings"
+)
+
+// Config struct just aggregate different types of configs in one place
+type Config struct {
+	Ingestor   *ingestor.Config `json:"ingestor"`
+	Scanner    *scanner.Config  `json:"scanner"`
+	StatusFile string           `json:"statusFile"`
+
+	// StateFile contains a path to file where scanner will keep its scan states
+	StateFile string `json:"stateFile"`
+}
+
+const (
+	cDefaultConfigStatus = "/opt/logrange/collector/status"
+	cDefaultStateFile    = "/opt/logrange/collector/state"
+)
+
+func NewDefaultConfig() *Config {
+	return &Config{
+		Ingestor:   ingestor.NewDefaultConfig(),
+		Scanner:    scanner.NewDefaultConfig(),
+		StatusFile: cDefaultConfigStatus,
+		StateFile:  cDefaultStateFile,
+	}
+}
+
+// LoadFromFile loads the config from file name provided in path
+func (ac *Config) LoadFromFile(path string) error {
+	data, err := ioutil.ReadFile(path)
+	if err != nil {
+		return err
+	}
+
+	toLower := strings.ToLower(path)
+	if strings.HasSuffix(toLower, "yaml") {
+		return yaml.Unmarshal(data, ac)
+	}
+	return json.Unmarshal(data, ac)
+}
+
+// Apply sets non-empty fields value from ac1 to ac
+func (ac *Config) Apply(ac1 *Config) {
+	if ac1 == nil {
+		return
+	}
+
+	ac.Ingestor.Apply(ac1.Ingestor)
+	ac.Scanner.Apply(ac1.Scanner)
+	if ac1.StatusFile != "" {
+		ac.StatusFile = ac1.StatusFile
+	}
+	if ac1.StateFile != "" {
+		ac.StateFile = ac1.StateFile
+	}
+}
diff --git a/pkg/collector/ingestor/config.go b/pkg/collector/ingestor/config.go
new file mode 100644
index 0000000..3e839db
--- /dev/null
+++ b/pkg/collector/ingestor/config.go
@@ -0,0 +1,87 @@
+package ingestor
+
+import (
+	"github.com/mohae/deepcopy"
+)
+
+type (
+	// Config struct bears the ingestor configuration information. The
+	// ingestor will use it for sending data to the aggragator. The structure
+	// is used for configuring the ingestor
+	Config struct {
+		Server           string          `json:"server"`
+		RetrySec         int             `json:"retrySec"`
+		HeartBeatMs      int             `json:"heartBeatMs"`
+		PacketMaxRecords int             `json:"packetMaxRecords"`
+		AccessKey        string          `json:"accessKey"`
+		SecretKey        string          `json:"secretKey"`
+		Schemas          []*SchemaConfig `json:"schemas"`
+	}
+
+	// SchemaConfig struct contains information by matching a file (PathMatcher)
+	// to the journal id (SourceId) it also contains tags that will be used
+	// for the match.
+	SchemaConfig struct {
+		PathMatcher string            `json:"pathMatcher"`
+		SourceId    string            `json:"sourceId"`
+		Tags        map[string]string `json:"tags"`
+	}
+)
+
+func NewDefaultConfig() *Config {
+	ic := new(Config)
+	ic.Server = "127.0.0.1:9966"
+	ic.RetrySec = 5
+	ic.PacketMaxRecords = 1000
+	ic.HeartBeatMs = 15000
+	ic.AccessKey = ""
+	ic.SecretKey = ""
+	ic.Schemas = NewDefaultSchemaConfigs()
+	return ic
+}
+
+func NewDefaultSchemaConfigs() []*SchemaConfig {
+	return []*SchemaConfig{
+		{
+			PathMatcher: "/*(?:.+/)*(?P<file>.+\\..+)",
+			SourceId:    "{file}",
+			Tags:        map[string]string{"file": "{file}"},
+		},
+	}
+}
+
+// Apply sets non-empty fields value from ic1 to ic
+func (ic *Config) Apply(ic1 *Config) {
+	if ic1 == nil {
+		return
+	}
+
+	if ic1.Server != "" {
+		ic.Server = ic1.Server
+	}
+
+	if ic1.RetrySec != 0 {
+		ic.RetrySec = ic1.RetrySec
+	}
+
+	if ic1.PacketMaxRecords != 0 {
+		ic.PacketMaxRecords = ic1.PacketMaxRecords
+	}
+
+	if ic1.HeartBeatMs != 0 {
+		ic.HeartBeatMs = ic1.HeartBeatMs
+	}
+
+	if ic1.AccessKey != "" {
+		ic.AccessKey = ic1.AccessKey
+	}
+
+	if ic1.SecretKey != "" {
+		ic.SecretKey = ic1.SecretKey
+	}
+
+	if len(ic1.Schemas) != 0 {
+		ic.Schemas = deepcopy.Copy(ic1.Schemas).([]*SchemaConfig)
+	}
+}
+
diff --git a/pkg/collector/ingestor/encoder.go b/pkg/collector/ingestor/encoder.go
new file mode 100644
index 0000000..d88acee
--- /dev/null
+++ b/pkg/collector/ingestor/encoder.go
@@ -0,0 +1,60 @@
+package ingestor
+
+import (
+	"github.com/logrange/logrange/pkg/collector/model"
+	"github.com/logrange/logrange/pkg/logevent"
+	"github.com/logrange/logrange/pkg/records/inmem"
+)
+
+// encoder structs intends to form a binary package will be send by Atmosphere
+type encoder struct {
+	bwriter inmem.Writer
+	buf []byte
+}
+
+// newEncoder creates a new encoder object
+func newEncoder() *encoder {
+	e := new(encoder)
+	e.buf = make([]byte, 4096)
+	e.bwriter.Reset(e.buf, true)
+	return e
+}
+
+// encode forms a binary package for sending it through a wire. It expects header
+// and a set of records in ev. As a result it returns slice of bytes or an error
+// if any
+func (e *encoder) encode(hdr *hdrsCacheRec, ev *model.Event) ([]byte, error) {
+	e.bwriter.Reset(e.buf, true)
+	bf, err := e.bwriter.Allocate(len(hdr.srcId), true)
+	if err != nil {
+		return nil, err
+	}
+
+	_, err = logevent.MarshalStringBuf(hdr.srcId, bf)
+	if err != nil {
+		return nil, err
+	}
+
+	first := true
+	var le logevent.LogEvent
+	for _, r := range ev.Records {
+		if first {
+			le.InitWithTagLine(int64(r.GetTs().UnixNano()), logevent.WeakString(r.Data), hdr.tags)
+		} else {
+			le.Init(int64(r.GetTs().UnixNano()), logevent.WeakString(r.Data))
+		}
+		first = false
+
+		rb, err := e.bwriter.Allocate(le.BufSize(), true)
+		if err != nil {
+			return nil, err
+		}
+		_, err = le.Marshal(rb)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	e.buf, err = e.bwriter.Close()
+	return e.buf, err
+}
diff --git a/pkg/collector/ingestor/ingestor.go b/pkg/collector/ingestor/ingestor.go
new file mode 100644
index 0000000..4b7a277
--- /dev/null
+++ b/pkg/collector/ingestor/ingestor.go
@@ -0,0 +1,329 @@
+// ingestor package contains a code which helps to build a data ingestor for the
+// aggregator
+package ingestor
+
+import (
+	"context"
+	"fmt"
+	"github.com/logrange/logrange/pkg/collector/model"
+	"github.com/logrange/logrange/pkg/dstruct"
+	"github.com/logrange/logrange/pkg/logevent"
+	"github.com/logrange/logrange/pkg/proto/atmosphere"
+	"github.com/logrange/logrange/pkg/util"
+	"regexp"
+	"regexp/syntax"
+	"strings"
+	"sync"
+	"time"
+
+	"github.com/jrivets/log4g"
+	"github.com/mohae/deepcopy"
+	"github.com/pkg/errors"
+)
+
+type (
+	// schema struct contains a schema descriptor, which includes the SchemaConfig
+	// and corresponding reg-exp matcher, which is going to be used for matching
+	// the schema. This structure is used internally for identifying the schema.
+	schema struct {
+		cfg     *SchemaConfig
+		matcher *regexp.Regexp
+	}
+
+	// Ingestor is used for sending data received from scanner to an log aggregator.
+	Ingestor struct {
+		cfg        *Config
+		aClient    atmosphere.Writer
+		pktEncoder *encoder
+		schemas    []*schema
+		logger     log4g.Logger
+		ctx        context.Context
+
+		lock sync.Mutex
+		// hdrsCache allows to cache headers by file name
+		hdrsCache *dstruct.Lru
+	}
+
+	// hdrsCacheRec struct is used by Ingestor to cache association between file
+	// name and the message data header.
+	hdrsCacheRec struct {
+		srcId string
+		tags  logevent.TagLine
+	}
+)
+
+func (hcr *hdrsCacheRec) String() string {
+	return fmt.Sprint("{srcId=", hcr.srcId, ", tags=", hcr.tags, "}")
+}
+
+func NewIngestor(cfg *Config, ctx context.Context) (*Ingestor, error) {
+	if err := checkConfig(cfg); err != nil {
+		return nil, err
+	}
+
+	logger := log4g.GetLogger("collector.ingestor")
+	logger.Info("Creating, config=", util.ToJsonStr(cfg))
+
+	ing := new(Ingestor)
+	ing.hdrsCache = dstruct.NewLru(10000, 5*time.Minute, nil)
+	ing.cfg = deepcopy.Copy(cfg).(*Config)
+	ing.pktEncoder = newEncoder()
+	ing.logger = logger
+	ing.ctx = ctx
+
+	ing.schemas = make([]*schema, 0, len(cfg.Schemas))
+	for _, s := range cfg.Schemas {
+		ing.schemas = append(ing.schemas, newSchema(s))
+	}
+
+	logger.Info("Created!")
+	return ing, nil
+}
+
+func (i *Ingestor) Run(ctx context.Context, events <-chan *model.Event) chan bool {
+	i.connect()
+	done := make(chan bool)
+	go func() {
+		defer close(done)
+		for ctx.Err() == nil {
+			select {
+			case <-ctx.Done():
+				return
+			case ev := <-events:
+				var err error
+				for ctx.Err() == nil {
+					if err == nil {
+						err = i.ingest(ev)
+						if err == nil {
+							ev.Confirm()
+							break
+						}
+					}
+					i.logger.Info("Ingestor error, recovering; cause: ", err)
+					err = i.connect()
+				}
+			}
+		}
+	}()
+	return done
+}
+
+func (i *Ingestor) GetKnownTags() map[interface{}]interface{} {
+	i.lock.Lock()
+	res := i.hdrsCache.GetData()
+	i.lock.Unlock()
+	return res
+}
+
+func (i *Ingestor) IsConnected() bool {
+	if i.aClient != nil {
+		return true
+	}
+	return false
+}
+
+func (i *Ingestor) connect() error {
+	i.logger.Info("Connecting to ", i.cfg.Server)
+	var (
+		acl atmosphere.Writer
+		err error
+	)
+
+	retry := time.Duration(i.cfg.RetrySec) * time.Second
+	for {
+		acl, err = atmosphere.NewClient(i.cfg.Server,
+			&atmosphere.ClientConfig{
+				HeartBeatMs: i.cfg.HeartBeatMs,
+				AccessKey: i.cfg.AccessKey,
+				SecretKey: i.cfg.SecretKey})
+		if err == nil {
+			break
+		}
+
+		i.logger.Warn("Could not connect to the server, err=", err, " will try in ", retry)
+		select {
+		case <-i.ctx.Done():
+			return fmt.Errorf("interrupted")
+		case <-time.After(retry):
+		}
+		i.logger.Warn("after 5 sec")
+	}
+
+	i.aClient = acl
+	i.logger.Info("connected")
+	return nil
+}
+
+func (i *Ingestor) ingest(ev *model.Event) error {
+	if i.aClient == nil {
+		return fmt.Errorf("not initialized")
+	}
+
+	header, err := i.getHeaderByFilename(ev.File)
+	if err != nil {
+		i.aClient = nil
+		return err
+	}
+
+	i.logger.Trace("Ingest header=", header, ", ev.File=", ev.File, ", len(ev.Records)=", len(ev.Records))
+	buf, err := i.pktEncoder.encode(header, ev)
+	if err != nil {
+		i.aClient = nil
+		return err
+	}
+	_, err = i.aClient.Write(buf, nil)
+	if err != nil {
+		i.aClient = nil
+		return err
+	}
+	return nil
+}
+
+// getHeaderByFilename get filename and forms header using schema and configuration
+// it can cache already calculated headers, so will work quickly this case
+func (i *Ingestor) getHeaderByFilename(filename string) (*hdrsCacheRec, error) {
+	i.lock.Lock()
+	defer i.lock.Unlock()
+
+	val := i.hdrsCache.Get(filename)
+	if val != nil {
+		hdr := val.Val().(*hdrsCacheRec)
+		return hdr, nil
+	}
+
+	schm := i.getSchema(filename)
+	if schm == nil {
+		return nil, errors.New("no schema found!")
+	}
+
+	vars := schm.getVars(filename)
+	tags := make(map[string]string, len(schm.cfg.Tags))
+
+	for k, v := range schm.cfg.Tags {
+		tags[k] = schm.subsVars(v, vars)
+	}
+
+	srcId := schm.subsVars(schm.cfg.SourceId, vars)
+	tm, err := logevent.NewTagMap(tags)
+	if err != nil {
+		return nil, err
+	}
+	hdr := &hdrsCacheRec{srcId, tm.BuildTagLine()}
+
+	i.hdrsCache.Put(filename, hdr, 1)
+	return hdr, nil
+}
+
+func (i *Ingestor) getSchema(filename string) *schema {
+	for _, s := range i.schemas {
+		if s.matcher.MatchString(filename) {
+			return s
+		}
+	}
+	return nil
+}
+
+func (i *Ingestor) close() {
+	i.logger.Info("Closing...")
+	if i.aClient != nil {
+		i.aClient.Close()
+	}
+	i.logger.Info("Closed.")
+}
+
+//=== schemaConfig
+
+func (s *SchemaConfig) String() string {
+	return util.ToJsonStr(s)
+}
+
+//=== schema
+
+func newSchema(cfg *SchemaConfig) *schema {
+	return &schema{
+		cfg:     cfg,
+		matcher: regexp.MustCompile(cfg.PathMatcher),
+	}
+}
+
+func (s *schema) getVars(l string) map[string]string {
+	names := s.matcher.SubexpNames()
+	match := s.matcher.FindStringSubmatch(l)
+
+	if len(names) > 1 {
+		names = names[1:] //skip ""
+	}
+	if len(match) > 1 {
+		match = match[1:] //skip "" value
+	}
+
+	vars := make(map[string]string, len(names))
+	for i, n := range names {
+		if len(match) > i {
+			vars[n] = match[i]
+		} else {
+			vars[n] = ""
+		}
+	}
+	return vars
+}
+
+func (s *schema) subsVars(l string, vars map[string]string) string {
+	for k, v := range vars {
+		l = strings.Replace(l, "{"+k+"}", v, -1)
+	}
+	return l
+}
+
+func (s *schema) String() string {
+	return util.ToJsonStr(s.cfg)
+}
+
+//=== helpers
+
+func checkConfig(cfg *Config) error {
+	if cfg == nil {
+		return fmt.Errorf("invalid config=%v", cfg)
+	}
+
+	if cfg.RetrySec < 1 {
+		return fmt.Errorf("invalid config; retry connect timeout=%d, expecting 1 second or more", cfg.RetrySec)
+	}
+
+	if strings.TrimSpace(cfg.Server) == "" {
+		return fmt.Errorf("invalid config; server=%v, must be non-empty", cfg.Server)
+	}
+
+	if cfg.HeartBeatMs < 100 {
+		return fmt.Errorf("invalid config; heartBeatMs=%v, must be >= 100ms", cfg.HeartBeatMs)
+	}
+
+	if cfg.PacketMaxRecords <= 0 {
+		return fmt.Errorf("invalid config; packetMaxRecords=%v, must be > 0", cfg.PacketMaxRecords)
+	}
+
+	if len(cfg.Schemas) == 0 {
+		return errors.New("invalid config; at least 1 schema must be defined")
+	}
+
+	for _, s := range cfg.Schemas {
+		if err := checkSchema(s); err != nil {
+			return fmt.Errorf("invalid config; invalid schema=%v, %v", s, err)
+		}
+	}
+	return nil
+}
+
+func checkSchema(s *SchemaConfig) error {
+	if strings.TrimSpace(s.PathMatcher) == "" {
+		return errors.New("patchMatcher must be non-empty")
+	}
+	_, err := syntax.Parse(s.PathMatcher, syntax.Perl)
+	if err != nil {
+		return fmt.Errorf("pathMatcher is invalid; %v", err)
+	}
+	if strings.TrimSpace(s.SourceId) == "" {
+		return errors.New("sourceId must be non-empty")
+	}
+	return nil
+}
diff --git a/pkg/collector/model/event.go b/pkg/collector/model/event.go
new file mode 100644
index 0000000..a5ab4c3
--- /dev/null
+++ b/pkg/collector/model/event.go
@@ -0,0 +1,52 @@
+package model
+
+import (
+	"encoding/json"
+	"github.com/logrange/logrange/pkg/util"
+)
+
+// Event is a structure which contains a list of records, parsed from the sourced file
+type Event struct {
+
+	// File contains filename where the records come from
+	File string `json:"file"`
+	// Records list of parsed record, that come from the file
+	Records []*Record `json:"records"`
+
+	// confCh is handling the event notification channel. The consumer,
+	// after handling the event must perform a READ operation from the channel,
+	// signalling the scanner about successful event handling transaction
+	confCh chan struct{}
+}
+
+func NewEvent(file string, recs []*Record, confCh chan struct{}) *Event {
+	return &Event{
+		File:    file,
+		Records: recs,
+		confCh:  confCh,
+	}
+}
+
+func (e *Event) MarshalJSON() ([]byte, error) {
+	type alias Event
+	return json.Marshal(&struct {
+		*alias
+		Records int `json:"records"`
+	}{
+		alias:   (*alias)(e),
+		Records: len(e.Records),
+	})
+}
+
+func (e *Event) Confirm() bool {
+	var ok bool
+	if e.confCh != nil {
+		_, ok = <-e.confCh
+		e.confCh = nil
+	}
+	return ok
+}
+
+func (e *Event) String() string {
+	return util.ToJsonStr(e)
+}
\ No newline at end of file
diff --git a/pkg/collector/model/record.go b/pkg/collector/model/record.go
new file mode 100644
index 0000000..0dfcde4
--- /dev/null
+++ b/pkg/collector/model/record.go
@@ -0,0 +1,46 @@
+package model
+
+import (
+	"time"
+)
+
+type Record struct {
+	Data []byte                 `json:"data"`
+	Meta map[string]interface{} `json:"meta"`
+}
+
+const TsField = "timestamp"
+
+func NewEmptyRecord() *Record {
+	return &Record{
+		Meta: map[string]interface{}{},
+	}
+}
+
+func (r *Record) GetData() []byte {
+	return r.Data
+}
+
+func (r *Record) SetData(data []byte) {
+	r.Data = data
+}
+
+func (r *Record) GetTs() time.Time {
+	if t := r.GetMeta(TsField); t != nil {
+		return t.(time.Time)
+	}
+	return time.Time{}
+}
+
+func (r *Record) SetTs(t time.Time) {
+	r.SetMeta(TsField, t)
+}
+
+func (r *Record) GetMeta(k string) interface{} {
+	v, _ := r.Meta[k]
+	return v
+}
+
+func (r *Record) SetMeta(k string, v interface{}) {
+	r.Meta[k] = v
+}
diff --git a/pkg/collector/scanner/dtparser/dtparser.go b/pkg/collector/scanner/dtparser/dtparser.go
new file mode 100644
index 0000000..134f7bd
--- /dev/null
+++ b/pkg/collector/scanner/dtparser/dtparser.go
@@ -0,0 +1,251 @@
+package dtparser
+
+import (
+	"fmt"
+	"regexp"
+	"strings"
+	"time"
+	"unsafe"
+)
+
+type (
+	Parser struct {
+		formats     []*ParserFormat
+		defLocation *time.Location
+	}
+
+	ParserFormat struct {
+		frmt        string
+		goFrmt      string
+		rExp        *regexp.Regexp
+		hasLocation bool
+		hasYear     bool
+		parser      *Parser
+	}
+
+	// term structure describes transformation from user friendly symbol to
+	// go-lang layout and part of regular expression that can be used
+	// for finding the symbol in a text. Plese see terms slice below.
+	term struct {
+		symbol string
+		layout string
+		expr   string
+	}
+)
+
+const (
+	tsGroup = "timestamp"
+)
+
+var (
+	KnownFormats = []string{
+		"MMM D, YYYY h:mm:ss P",
+		"DDD MMM _D HH:mm:ss YYYY",
+		"DDD MMM _D HH:mm:ss MST YYYY",
+		"DDD MMM DD HH:mm:ss ZZZZ YYYY",
+		"DDDD, YY-MMM-DD HH:mm:ss ZZZ",
+		"DDD, DD MMM YYYY HH:mm:ss ZZZ",
+		"DDD, DD MMM YYYY HH:mm:ss ZZZZ",
+		"DDD, D MMM YYYY HH:mm:ss ZZZZ",
+		"DD MMM YYYY, HH:mm",
+		"YYYY-MMM-DD",
+		"DD MMMM YYYY",
+
+		// mm/dd/yy
+		"DD/MM/YYYY HH:mm:ss.SSS",
+		"DD/MM/YYYY HH:mm:ss",
+		"D/MM/YYYY HH:mm:ss",
+		"DD/M/YYYY HH:mm:ss",
+		"D/M/YYYY HH:mm:ss",
+		"D/M/YYYY hh:mm:ss P",
+		"DD/MM/YYYY HH:mm",
+		"D/M/YYYY HH:mm",
+		"D/M/YY HH:mm",
+		"D/M/YYYY hh:mm P",
+		"D/M/YYYY h:mm P",
+		"DD/MMM/YYYY:HH:mm:ss ZZZZ",
+		"DD/MM/YYYY",
+		"D/MM/YYYY",
+		"DD/MM/YY",
+		"D/M/YY",
+
+		// yyyy/mm/dd
+		"YYYY/MM/DD HH:mm:ss.SSS",
+		"YYYY/MM/DD HH:mm:ss",
+		"YYYY/MM/D HH:mm:ss",
+		"YYYY/M/DD HH:mm:ss",
+		"YYYY/MM/DD HH:mm",
+		"YYYY/M/D HH:mm",
+		"YYYY/MM/DD",
+		"YYYY/M/DD",
+
+		// yyyy-mm-ddThh
+		"YYYY-MM-DDTHH:mm:ss.SSSZZZZ",
+		"YYYY-MM-DDTHH:mm:ss.SSSZ",
+		"YYYY-MM-DDTHH:mm:ssZZZZZ",
+		"YYYY-MM-DDTHH:mm:ssZZZZ",
+		"YYYY-MM-DDTHH:mm:ssZ",
+		"YYYY-MM-DDTHH:mm:ss",
+
+		// yyyy-mm-dd hh:mm:ss
+		"YYYY-MM-DD HH:mm:ss.SSS ZZZZ ZZZ",
+		"YYYY-MM-DD HH:mm:ss.SSS",
+		"YYYY-MM-DD HH:mm:ss ZZZZZ",
+		"YYYY-MM-DD HH:mm:ssZZZZZ",
+		"YYYY-MM-DD HH:mm:ss ZZZZ ZZZ",
+		"YYYY-MM-DD HH:mm:ss ZZZZ",
+		"YYYY-MM-DD HH:mm:ss ZZZ",
+		"YYYY-MM-DD hh:mm:ss P",
+		"YYYY-MM-DD HH:mm:ss",
+		"YYYY-MM-DD HH:mm",
+		"YYYY-MM-DD",
+
+		// mm.dd.yy
+		"MM.DD.YYYY",
+		"MM.DD.YY",
+
+		// no year
+		"DDD MMM _D HH:mm:ss.SSS",
+		"DDD MMM DD HH:mm:ss.SSS",
+		"MMM DD HH:mm:ss",
+		"MMM _D HH:mm:ss",
+	}
+
+	terms = []term{ //descending order of the 'alike' symbols is important
+		{"YYYY", "2006", "[1-2]\\d{3}"},
+		{"YY", "06", "\\d{2}"},
+		{"MMMM", "January", "[A-Z][a-z]{2,8}"},
+		{"MMM", "Jan", "[A-Z][a-z]{2}"},
+		{"MM", "01", "[0-3]\\d"},
+		{"M", "1", "\\d{1,2}"},
+		{"DDDD", "Monday", "[A-Z][a-z]{5,7}"},
+		{"DDD", "Mon", "[A-Z][a-z]{2}"},
+		{"DD", "02", "\\d{2}"},
+		{"_D", "_2", "(?: \\d{1}|\\d{2})"},
+		{"D", "2", "\\d{1,2}"},
+		{"HH", "15", "\\d{2}"},
+		{"hh", "03", "\\d{2}"},
+		{"h", "3", "\\d{1,2}"},
+		{"mm", "04", "\\d{2}"},
+		{"m", "4", "\\d{1,2}"},
+		{"ss", "05", "\\d{2}"},
+		{"s", "5", "\\d{1,2}"},
+		{".SSS", ".999999999", ".\\d{3,}"},
+		{"P", "PM", "(?:am|AM|pm|PM)"},
+		{"ZZZZZ", "-07:00", "[+-][0-9]{2}:[0-9]{2}"},
+		{"ZZZZ", "-0700", "[+-][0-9]{4}"},
+		{"ZZZ", "MST", "[A-Z]{3}"},
+		{"ZZ", "Z07:00", "Z[0-9]{2}:[0-9]{2}"},
+	}
+
+	ErrNotMatched = fmt.Errorf("not matched")
+)
+
+func NewDefaultParser() *Parser {
+	return NewParser(KnownFormats)
+}
+
+func NewParser(frmts []string) *Parser {
+	p := new(Parser)
+	p.formats = make([]*ParserFormat, len(frmts))
+	p.defLocation = time.UTC
+
+	for i, frmt := range frmts {
+		hasLocation := false
+		if strings.Contains(frmt, "Z") {
+			hasLocation = true
+		}
+
+		hasYear := false
+		if strings.Contains(frmt, "Y") {
+			hasYear = true
+		}
+
+		lineFmt := lmap(frmt)
+		re := regexp.MustCompile(fmt.Sprintf("(?P<%v>%v)", tsGroup, emap(frmt)))
+		grps := re.SubexpNames()
+		if len(grps) < 2 || grps[1] != tsGroup {
+			panic(fmt.Sprint("wrong regular expression ", lineFmt, " which doesn't have ", tsGroup, " group, or has many. grps=", grps, len(grps)))
+		}
+
+		p.formats[i] = &ParserFormat{
+			frmt:        frmt,
+			goFrmt:      lineFmt,
+			rExp:        re,
+			parser:      p,
+			hasLocation: hasLocation,
+			hasYear:     hasYear,
+		}
+	}
+	return p
+}
+
+func (p *Parser) SetDefaultLocation(dl *time.Location) {
+	p.defLocation = dl
+}
+
+func (p *Parser) Parse(buf []byte, ignorePF *ParserFormat) (time.Time, *ParserFormat) {
+	for i, pf := range p.formats {
+		if pf == ignorePF {
+			continue
+		}
+
+		tm, err := pf.Parse(buf)
+		if err == nil {
+			if i > 0 {
+				copy(p.formats[1:i+1], p.formats[:i])
+				p.formats[0] = pf
+			}
+			return tm, pf
+		}
+	}
+	return time.Time{}, nil
+}
+
+func (pf *ParserFormat) Parse(buf []byte) (tm time.Time, err error) {
+	match := pf.rExp.FindSubmatch(buf)
+	if len(match) < 2 {
+		return tm, ErrNotMatched
+	}
+	str := *(*string)(unsafe.Pointer(&match[1]))
+
+	if pf.hasLocation {
+		tm, err = time.Parse(pf.goFrmt, str)
+	} else {
+		tm, err = time.ParseInLocation(pf.goFrmt, str, pf.parser.defLocation)
+	}
+	if err != nil {
+		return tm, err
+	}
+
+	if !pf.hasYear {
+		now := time.Now()
+		year := now.Year()
+		if tm.Month() > now.Month() {
+			year--
+		}
+		tm = time.Date(year, tm.Month(), tm.Day(), tm.Hour(), tm.Minute(), tm.Second(), tm.Nanosecond(), tm.Location())
+	}
+	return tm, nil
+}
+
+func (pf *ParserFormat) GetFormat() string {
+	return pf.frmt
+}
+
+// lmap builds human readable fromat transformation to go-lang format representation
+func lmap(format string) string {
+	layout := format
+	for _, t := range terms {
+		layout = strings.Replace(layout, t.symbol, t.layout, -1)
+	}
+	return layout
+}
+
+func emap(format string) string {
+	re := format
+	for _, t := range terms {
+		re = strings.Replace(re, t.symbol, t.expr, -1)
+	}
+	return re
+}
diff --git a/pkg/collector/scanner/dtparser/dtparser_test.go b/pkg/collector/scanner/dtparser/dtparser_test.go
new file mode 100644
index 0000000..4dd55d1
--- /dev/null
+++ b/pkg/collector/scanner/dtparser/dtparser_test.go
@@ -0,0 +1,13 @@
+package dtparser
+
+import (
+	"testing"
+)
+
+func TestParser(t *testing.T) {
+	p := NewDefaultParser()
+	_, pf := p.Parse([]byte("jopasdf govnoa Tue Jan 30 00:42:28.694 <kernel> Setting BTCoex Config: enable_2G:1, profile_2g:0, enable_5G:1, profile_5G:0"), nil)
+	if pf == nil {
+		t.Fatal("Must found :(, but could not")
+	}
+}
diff --git a/pkg/collector/scanner/json_parser.go b/pkg/collector/scanner/json_parser.go
new file mode 100644
index 0000000..af5d0ae
--- /dev/null
+++ b/pkg/collector/scanner/json_parser.go
@@ -0,0 +1,97 @@
+package scanner
+
+import (
+	"context"
+	"encoding/json"
+	"github.com/logrange/logrange/pkg/collector/model"
+	"io"
+	"os"
+	"time"
+	"unsafe"
+)
+
+type (
+	// jsonParser implements Parser, for reading lines(strings
+	// with /n separation) from a text file, treating every of the lines
+	// as a json message.
+	//
+	//This parser works ok with standard k8s logs output as
+	// a line per record, but in long-term prospective it needs to be reworked
+	// to address the restrictions it has now.
+	jsonParser struct {
+		fname    string
+		fReader  *os.File
+		lnReader *lineReader
+		pos      int64
+	}
+
+	// jsonRec defines format of every line, which has the object JSON enconding.
+	jsonRec struct {
+		Log    string    `json:"log"`
+		Stream string    `json:"stream"`
+		Time   time.Time `json:"time"`
+	}
+)
+
+func newJsonParser(fileName string, maxRecSize int, ctx context.Context) (*jsonParser, error) {
+	f, err := os.Open(fileName)
+	if err != nil {
+		return nil, err
+	}
+
+	jp := new(jsonParser)
+	jp.fname = fileName
+	jp.fReader = f
+	jp.lnReader = newLineReader(f, maxRecSize, ctx)
+	return jp, nil
+}
+
+func (jp *jsonParser) NextRecord() (*model.Record, error) {
+	line, err := jp.lnReader.readLine()
+	if err != nil {
+		return nil, err
+	}
+
+	var r jsonRec
+	err = json.Unmarshal(line, &r)
+	if err != nil {
+		return nil, err
+	}
+
+	rec := model.NewEmptyRecord()
+	rec.Data = *(*[]byte)(unsafe.Pointer(&r.Log))
+	rec.Meta["stream"] = r.Stream
+	rec.SetTs(r.Time)
+
+	jp.pos += int64(len(line))
+	return rec, nil
+}
+
+func (jp *jsonParser) SetStreamPos(pos int64) error {
+	if _, err := jp.fReader.Seek(pos, io.SeekStart); err != nil {
+		return err
+	}
+	jp.pos = pos
+	jp.lnReader.reset(jp.fReader)
+	return nil
+}
+
+func (jp *jsonParser) GetStreamPos() int64 {
+	return jp.pos
+}
+
+func (jp *jsonParser) Close() error {
+	jp.lnReader.Close()
+	return jp.fReader.Close()
+}
+
+func (jp *jsonParser) GetStats() *ParserStats {
+	ps := &ParserStats{}
+	ps.DataType = "JSON"
+	ps.Pos = jp.GetStreamPos()
+	fi, _ := jp.fReader.Stat()
+	if fi != nil {
+		ps.Size = fi.Size()
+	}
+	return ps
+}
diff --git a/pkg/collector/scanner/json_parser_test.go b/pkg/collector/scanner/json_parser_test.go
new file mode 100644
index 0000000..6165e31
--- /dev/null
+++ b/pkg/collector/scanner/json_parser_test.go
@@ -0,0 +1,71 @@
+package scanner
+
+import (
+	"context"
+	"io"
+	"path/filepath"
+	"reflect"
+	"runtime"
+	"testing"
+	"time"
+)
+
+func TestKnown(t *testing.T) {
+	_, d, _, _ := runtime.Caller(0)
+	tstFile := filepath.Dir(d) + "/testdata/logs/k8s/var/log/containers/kube-dns-5c47645d88-p7fpl_kube-system_dnsmasq-74d2de2d0542c5b507e57d5077832b2400273554c3c16d674f1302bc010d5a0e.log"
+
+	p, err := newJsonParser(tstFile, 1000000, context.Background())
+	if err != nil {
+		t.Fatal("Should be able to create the json parser, but got err=", err, " tstFile=", tstFile)
+	}
+	defer p.Close()
+
+	tm, _ := time.Parse(time.RFC3339Nano, "2018-02-06T10:22:49.201168034Z")
+
+	r, err := p.NextRecord()
+	if err != nil {
+		t.Fatal("should be able to parse")
+	}
+
+	if r.GetTs() != tm {
+		t.Fatal("Expecting ", tm, " but got ", r.GetTs())
+	}
+
+	pos := p.GetStreamPos()
+	r2, err := p.NextRecord()
+	pos2 := p.GetStreamPos()
+	if err != nil || pos2 <= pos {
+		t.Fatal("Could not read second record, or wrong pos is returned err=", err, ", pos=", pos, ", r2 pos2=", pos2)
+	}
+
+	p.SetStreamPos(pos)
+	r22, err := p.NextRecord()
+	if err != nil || pos2 != p.GetStreamPos() {
+		t.Fatal("second record re-read, or err=", err, " pos2=", pos2, ", but returned ", p.GetStreamPos())
+	}
+
+	if !reflect.DeepEqual(r2, r22) {
+		t.Fatal("r2=", r2, " is not the same as ", r22)
+	}
+}
+
+func TestReadAll(t *testing.T) {
+	_, d, _, _ := runtime.Caller(0)
+	tstFile := filepath.Dir(d) + "/testdata/logs/k8s/var/log/containers/kube-dns-5c47645d88-p7fpl_kube-system_dnsmasq-74d2de2d0542c5b507e57d5077832b2400273554c3c16d674f1302bc010d5a0e.log"
+
+	p, err := newJsonParser(tstFile, 1000000, context.Background())
+	if err != nil {
+		t.Fatal("Should be able to create the json parser, but got err=", err, " tstFile=", tstFile)
+	}
+	defer p.Close()
+
+	cnt := 0
+	for err == nil {
+		_, err = p.NextRecord()
+		cnt++
+	}
+
+	if err != io.EOF || cnt != 17 {
+		t.Fatal("expection io.EOF and 17 records, but err=", err, " and cnt=", cnt)
+	}
+}
diff --git a/pkg/collector/scanner/line_reader.go b/pkg/collector/scanner/line_reader.go
new file mode 100644
index 0000000..67550d6
--- /dev/null
+++ b/pkg/collector/scanner/line_reader.go
@@ -0,0 +1,87 @@
+package scanner
+
+import (
+	"bufio"
+	"context"
+	"github.com/logrange/logrange/pkg/util"
+	"io"
+	"time"
+)
+
+type (
+	lineReader struct {
+		ctx    context.Context
+		cancel context.CancelFunc
+		r      *bufio.Reader
+		eofTO  time.Duration
+	}
+)
+
+const (
+	cSleepWhenEOFMs   = 200 * time.Millisecond
+)
+
+func newLineReader(ioRdr io.Reader, bufSize int, ctx context.Context) *lineReader {
+	r := new(lineReader)
+	r.r = bufio.NewReaderSize(ioRdr, bufSize)
+	r.ctx, r.cancel = context.WithCancel(ctx)
+	r.eofTO = cSleepWhenEOFMs
+	return r
+}
+
+// readLine reads lines from provided reader until EOF is met. It follows the
+// io.Reader.Read contract and returns io.EOF only when it doesn't have data to be
+// read.
+func (r *lineReader) readLine() ([]byte, error) {
+	var buf []byte
+	for r.ctx.Err() == nil {
+		line, err := r.r.ReadSlice('\n')
+		line = util.BytesCopy(line)
+		if err == nil {
+			return concatBufs(buf, line), err
+		}
+
+		if err == io.EOF {
+			buf = concatBufs(buf, line)
+			if len(buf) == 0 {
+				return nil, io.EOF
+			}
+			r.sleep(r.eofTO)
+			continue
+		}
+
+		if err == bufio.ErrBufferFull {
+			return concatBufs(buf, line), nil
+		}
+		return nil, err
+	}
+	return nil, io.ErrClosedPipe
+}
+
+func (r *lineReader) reset(ioRdr io.Reader) {
+	r.r.Reset(ioRdr)
+}
+
+func (r *lineReader) sleep(to time.Duration) {
+	select {
+	case <-r.ctx.Done():
+		return
+	case <-time.After(to):
+		return
+	}
+}
+
+func (r *lineReader) Close() error {
+	r.cancel()
+	return nil
+}
+
+func concatBufs(b1, b2 []byte) []byte {
+	if len(b1) == 0 {
+		return b2
+	}
+	nb := make([]byte, len(b1)+len(b2))
+	copy(nb[:len(b1)], b1)
+	copy(nb[len(b1):], b2)
+	return nb
+}
diff --git a/pkg/collector/scanner/lines_parser.go b/pkg/collector/scanner/lines_parser.go
new file mode 100644
index 0000000..cd2d436
--- /dev/null
+++ b/pkg/collector/scanner/lines_parser.go
@@ -0,0 +1,79 @@
+package scanner
+
+import (
+	"context"
+	"github.com/logrange/logrange/pkg/collector/model"
+	"io"
+	"os"
+)
+
+type (
+	// lines_parser implements parser, for reading from a file lines(strings
+	// with /n separation) one by one and transforms them to records
+	linesParser struct {
+		fname    string
+		fReader  *os.File
+		lnReader *lineReader
+		lnParser *stringParser
+		pos      int64
+	}
+)
+
+func newLinesParser(fileName string, dtFormats []string, maxRecSize int, ctx context.Context) (*linesParser, error) {
+	lnParser := newStringParser(dtFormats...)
+
+	f, err := os.Open(fileName)
+	if err != nil {
+		return nil, err
+	}
+
+	lp := new(linesParser)
+	lp.fname = fileName
+	lp.fReader = f
+	lp.lnReader = newLineReader(f, maxRecSize, ctx)
+	lp.lnParser = lnParser
+	return lp, nil
+}
+
+func (lp *linesParser) NextRecord() (*model.Record, error) {
+	line, err := lp.lnReader.readLine()
+	if err != nil {
+		return nil, err
+	}
+	lp.pos += int64(len(line))
+	return lp.lnParser.apply(line), nil
+}
+
+func (lp *linesParser) SetStreamPos(pos int64) error {
+	if _, err := lp.fReader.Seek(pos, io.SeekStart); err != nil {
+		return err
+	}
+	lp.pos = pos
+	lp.lnReader.reset(lp.fReader)
+	return nil
+}
+
+func (lp *linesParser) GetStreamPos() int64 {
+	return lp.pos
+}
+
+func (lp *linesParser) Close() error {
+	lp.lnReader.Close()
+	return lp.fReader.Close()
+}
+
+func (lp *linesParser) GetStats() *ParserStats {
+	ps := &ParserStats{}
+	ps.DataType = "TEXT"
+	ps.DateFormats = make(map[string]int64)
+	ps.Size = -1 // just in case of an error
+	for k, v := range lp.lnParser.stats.hits {
+		ps.DateFormats[k] = v
+	}
+	ps.Pos = lp.GetStreamPos()
+	fi, err := os.Stat(lp.fname)
+	if err == nil {
+		ps.Size = fi.Size()
+	}
+	return ps
+}
diff --git a/pkg/collector/scanner/parser.go b/pkg/collector/scanner/parser.go
new file mode 100644
index 0000000..344aa22
--- /dev/null
+++ b/pkg/collector/scanner/parser.go
@@ -0,0 +1,41 @@
+package scanner
+
+import (
+	"github.com/logrange/logrange/pkg/collector/model"
+	"io"
+)
+
+type (
+	// Parser provides an interface for retrieving records from a data-stream.
+	// Implementations of the interface are supposed to be initialized by the
+	// stream (io.Reader)
+	Parser interface {
+		io.Closer
+
+		// NextRecord parses next record. It returns error if it could not parse
+		// a record from the stream. io.EOF is returned if no new records found, but
+		// end is reached.
+		NextRecord() (*model.Record, error)
+
+		// SetStreamPos specifies the stream position for the next record read
+		SetStreamPos(pos int64) error
+
+		// GetStreamPos returns position of the last successfully (error was nil)
+		// returned record by nextRecord(). If nextRecord() returned non-nil
+		// error the getStreamPos() returned value is not relevant and should
+		// not be used as a valid stream position.
+		GetStreamPos() int64
+
+		// GetStat returns the parser statistic
+		GetStats() *ParserStats
+	}
+
+	// ParserStat struct contains information about the parser statistics
+	ParserStats struct {
+		DataType string
+		Size     int64
+		Pos      int64
+		// for text parsers provides information about found date-time formats
+		DateFormats map[string]int64
+	}
+)
diff --git a/pkg/collector/scanner/scanner.go b/pkg/collector/scanner/scanner.go
new file mode 100644
index 0000000..9bfbf49
--- /dev/null
+++ b/pkg/collector/scanner/scanner.go
@@ -0,0 +1,652 @@
+package scanner
+
+import (
+	"context"
+	"encoding/json"
+	"errors"
+	"fmt"
+	"github.com/logrange/logrange/pkg/collector/model"
+	"github.com/logrange/logrange/pkg/util"
+	"os"
+	"regexp"
+	"regexp/syntax"
+	"sort"
+	"strings"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/jrivets/log4g"
+	"github.com/mohae/deepcopy"
+)
+
+type (
+	// Config is a structure which contains scanner configuration and settings
+	Config struct {
+		// ScanPaths contains a list of path that should be scanned and check
+		// for potential sources. Scan Paths are defined like Globs, for example:
+		// /var/log/*.log etc.
+		ScanPaths            []string `json:"scanPaths"`
+		ScanPathsIntervalSec int      `json:"scanPathsIntervalSec"`
+
+		// Exclude contains slice of regular expressions (the slice could be empty)
+		// which will be applied to files found by ScanPaths. The Exclude slice
+		// of reg-exps intended to exclude some files which could be scanned.
+		Exclude []string `json:"exclude"`
+
+		// StateFlushIntervalSec defines the time interval to store the state
+		StateFlushIntervalSec int `json:"stateFlushIntervalSec"`
+
+		// FileFormats allows to specify which date-time formats can be used
+		// when parsing files by the specific matcher
+		FileFormats []*FileFormat `json:"fileFormats"`
+
+		// RecordMaxSizeBytes defines a maximum size of one record
+		RecordMaxSizeBytes int `json:"recordMaxSizeBytes"`
+
+		// EventMaxRecords defines how many records can be packed in one event
+		EventMaxRecords     int `json:"eventMaxRecords"`
+		EventSendIntervalMs int `json:"eventSendIntervalMs"`
+	}
+
+	// FileFormat defines a pattern of file matching and time formats, if any,
+	// that should be applied to the files when parsing
+	FileFormat struct {
+		// PathMatcher defines regExp which should be applied to a file name
+		// to identify whether the time format below will be applied to it or not
+		// example is '.*'
+		PathMatcher string `json:"pathMatcher"`
+
+		// DataFormat defines how data is formatted in the file: text, json etc.
+		// By the field value a type of parser for the worker will be created then
+		DataFormat string `json:"format"`
+
+		// TimeFormats contains a list of supposed date-time formats that should
+		// be tried for parsing the file lines (see dtparser) example is
+		// ["MMM D, YYYY h:mm:ss P"]
+		TimeFormats []string `json:"timeFormats"`
+	}
+
+	desc struct {
+		Id   string `json:"id"`
+		File string `json:"file"`
+
+		// The Scan size is used to catch up if the file size was changed and
+		// if it is reduced for later scans, the file was truncated and the
+		// offset will be reset. Please see getDescsToScan()
+		ScanSize int64 `json:"scanSize"`
+		Offset   int64 `json:"offset"`
+	}
+
+	descs map[string]*desc
+
+	workers map[string]*worker
+
+	Scanner struct {
+		cfg       *Config
+		ctx       context.Context
+		cancel    context.CancelFunc
+		logger    log4g.Logger
+		stStorage StateStorage
+
+		events  chan *model.Event
+		descs   atomic.Value
+		workers atomic.Value
+
+		// tracks list of files which were excluded for reporting purposes only
+		// contains []string
+		excludes atomic.Value
+		stopWg   sync.WaitGroup
+		lock     sync.Mutex
+		wrkId    int32
+	}
+
+	CollectorStats struct {
+		Config *Config
+		// contains list of files that were excluded from processing
+		Excludes []string
+		Workers  []*WorkerStats
+	}
+)
+
+const (
+	cTxtDataFormat  = "text"
+	cJsonDataFormat = "json"
+)
+
+func NewDefaultConfig() *Config {
+	return &Config{
+		ScanPaths:             []string{"/var/log/*.log", "/var/log/*/*.log"},
+		ScanPathsIntervalSec:  5,
+		StateFlushIntervalSec: 5,
+		RecordMaxSizeBytes:    16 * 1024,
+		EventMaxRecords:       1000,
+		EventSendIntervalMs:   200,
+	}
+}
+
+func (c *Config) String() string {
+	return util.ToJsonStr(c)
+}
+
+func (c *Config) Apply(c1 *Config) {
+	if c1 == nil {
+		return
+	}
+	if len(c1.ScanPaths) != 0 {
+		c.ScanPaths = deepcopy.Copy(c1.ScanPaths).([]string)
+	}
+	if c1.ScanPathsIntervalSec != 0 {
+		c.ScanPathsIntervalSec = c1.ScanPathsIntervalSec
+	}
+	if c1.StateFlushIntervalSec != 0 {
+		c.StateFlushIntervalSec = c1.StateFlushIntervalSec
+	}
+	if len(c1.FileFormats) != 0 {
+		c.FileFormats = deepcopy.Copy(c1.FileFormats).([]*FileFormat)
+	}
+	if c1.RecordMaxSizeBytes != 0 {
+		c.RecordMaxSizeBytes = c1.RecordMaxSizeBytes
+	}
+	if c1.EventMaxRecords != 0 {
+		c.EventMaxRecords = c1.EventMaxRecords
+	}
+	if c1.EventSendIntervalMs != 0 {
+		c.EventSendIntervalMs = c1.EventSendIntervalMs
+	}
+}
+
+//=== fileFormat
+func (c *FileFormat) String() string {
+	return util.ToJsonStr(c)
+}
+
+//=== scanner
+func NewScanner(cfg *Config, ss StateStorage) (*Scanner, error) {
+	if err := checkCfg(cfg); err != nil {
+		return nil, err
+	}
+
+	c := new(Scanner)
+	c.cfg = cfg
+	c.workers.Store(make(workers))
+	c.descs.Store(newDescs())
+	c.excludes.Store([]string{})
+	c.logger = log4g.GetLogger("collector.scanner")
+	c.stStorage = ss
+	return c, nil
+}
+
+func (s *Scanner) Start() error {
+	s.lock.Lock()
+	defer s.lock.Unlock()
+
+	if s.ctx != nil && s.ctx.Err() == nil {
+		return fmt.Errorf("wrong state, the component is already running")
+	}
+	s.ctx, s.cancel = context.WithCancel(context.Background())
+	s.events = make(chan *model.Event)
+
+	s.logger.Info("Starting, config=", s.cfg)
+
+	if err := s.loadStatesBeforeRun(); err != nil {
+		s.logger.Warn("could not start collector, err=", err)
+		s.stopInternal()
+		return err
+	}
+
+	s.runScanPaths()
+	s.runFlushState()
+
+	s.logger.Info("Started!")
+	return nil
+}
+
+func (s *Scanner) Stop() error {
+	s.lock.Lock()
+	defer s.lock.Unlock()
+
+	s.logger.Info("Stopping...")
+	s.stopInternal()
+	s.logger.Info("Stopped.")
+	return nil
+}
+
+func (s *Scanner) stopInternal() {
+	s.cancel()
+	s.stopWg.Wait()
+	close(s.events)
+	s.ctx = nil
+}
+
+func (s *Scanner) Events() <-chan *model.Event {
+	return s.events
+}
+
+func (s *Scanner) GetStats() *CollectorStats {
+	cs := new(CollectorStats)
+	cs.Config = new(Config)
+	*cs.Config = *s.cfg
+	cs.Excludes = s.excludes.Load().([]string)
+	wkrs := s.workers.Load().(workers)
+	cs.Workers = make([]*WorkerStats, 0, len(wkrs))
+	for _, wkr := range wkrs {
+		ws := wkr.GetStats()
+		idx := sort.Search(len(cs.Workers), func(pos int) bool {
+			return cs.Workers[pos].Filename >= ws.Filename
+		})
+		cs.Workers = append(cs.Workers, ws)
+		if idx < len(cs.Workers)-1 {
+			copy(cs.Workers[idx+1:], cs.Workers[idx:])
+		}
+		cs.Workers[idx] = ws
+
+	}
+	return cs
+}
+
+func (s *Scanner) loadStatesBeforeRun() error {
+	s.updateWorkersList(newDescs())
+	ds1, err := s.loadState()
+	if err != nil {
+		return err
+	}
+	ds2 := s.getDescsToScan()
+	s.logger.Info("loadStatesBeforeRun: ", len(ds2), " files were found by scan and ", len(ds1), " files states were read from state file.")
+	s.logger.Debug("loadStatesBeforeRun: From state file ", ds1)
+	s.logger.Debug("loadStatesBeforeRun: Found by scan ", ds2)
+	s.replaceOrRotate(ds1, ds2)
+	s.updateWorkersList(ds2)
+	s.logger.Info(len(ds2), " sources found, will scan them...")
+	return nil
+}
+
+func (s *Scanner) runScanPaths() {
+	s.stopWg.Add(1)
+	go func() {
+		s.logger.Info("Start scanning paths every ", s.cfg.ScanPathsIntervalSec, " seconds...")
+		defer func() {
+			s.logger.Info("Stop scanning paths.")
+			s.stopWg.Done()
+		}()
+
+		ticker := time.NewTicker(time.Second *
+			time.Duration(s.cfg.ScanPathsIntervalSec))
+
+		for s.wait(ticker) {
+			// existing descriptors are in ds1
+			ds1 := s.descs.Load().(descs)
+			// still found are in ds2
+			ds2 := s.getDescsToScan()
+			if chngs := s.replaceOrRotate(ds1, ds2); chngs > 0 {
+				s.logger.Info("scan procedure: ", chngs, " changes were made. old descs=", len(ds1), ", new descs=", len(ds2))
+			}
+			s.updateWorkersList(ds2)
+		}
+	}()
+}
+
+func (s *Scanner) getWorkerConfig(d *desc) (*workerConfig, error) {
+	wc := &workerConfig{
+		desc:             d,
+		EventMaxRecCount: s.cfg.EventMaxRecords,
+	}
+	wc.ctx, wc.cancel = context.WithCancel(s.ctx)
+	wc.logger = log4g.GetLogger("collector.scanner.worker").
+		WithId(fmt.Sprintf("{%d}", atomic.AddInt32(&s.wrkId, 1))).(log4g.Logger)
+
+	df := getDataFormat(d, s.cfg.FileFormats)
+	var err error
+	switch df {
+	case cJsonDataFormat:
+		wc.parser, err = newJsonParser(d.File, s.cfg.RecordMaxSizeBytes, wc.ctx)
+	case "":
+		fallthrough
+	case cTxtDataFormat:
+		wc.parser, err = newLinesParser(d.File, getTimeFormats(d, s.cfg.FileFormats), s.cfg.RecordMaxSizeBytes, wc.ctx)
+	default:
+		return nil, fmt.Errorf("Unsupported data format=%s expecting %s or %s", df, cTxtDataFormat, cJsonDataFormat)
+	}
+	return wc, err
+}
+
+func (s *Scanner) updateWorkersList(ds descs) {
+	newWks := make(workers)
+	oldWks := s.workers.Load().(workers)
+	for id, d := range ds {
+		w, ok := oldWks[id]
+
+		if ok && d != w.desc {
+			// descriptor was re-created, will re-scan
+			w.Close()
+		}
+
+		if !ok || w.isClosed() {
+			wc, err := s.getWorkerConfig(d)
+			if err != nil {
+				s.logger.Error("Could not create new worker by the descriptor ", d, ", err=", err)
+				continue
+			}
+
+			w = newWorker(wc)
+			s.stopWg.Add(1)
+			go func(w *worker) {
+				defer s.stopWg.Done()
+				err := w.run(s.sendEvent)
+				if err != nil {
+					s.logger.Warn("Worker ", w.desc, " reports err=", err)
+				}
+				w.Close()
+			}(w)
+		}
+		newWks[id] = w
+	}
+
+	for id, w := range oldWks {
+		if _, ok := newWks[id]; !ok {
+			if !w.isClosed() {
+				w.stopWhenEof()
+				newWks[id] = w
+			}
+		}
+	}
+
+	s.workers.Store(newWks)
+	s.descs.Store(ds)
+}
+
+func (s *Scanner) sendEvent(ev *model.Event) error {
+	select {
+	case <-s.ctx.Done():
+		return fmt.Errorf("Closed")
+	case s.events <- ev:
+	}
+	return nil
+}
+
+func (s *Scanner) runFlushState() {
+	s.stopWg.Add(1)
+	go func() {
+		s.logger.Info("Start flushing state every ", s.cfg.StateFlushIntervalSec, " seconds...")
+		defer func() {
+			s.logger.Info("Stop flushing state.")
+			s.flushState()
+			s.stopWg.Done()
+		}()
+
+		ticker := time.NewTicker(time.Second *
+			time.Duration(s.cfg.StateFlushIntervalSec))
+
+		for s.wait(ticker) {
+			s.logger.Debug("Flushing state=", s.descs.Load())
+			if err := s.flushState(); err != nil {
+				s.logger.Error("Unable to flush state, cause=", err)
+			}
+		}
+	}()
+}
+
+// flushState stores state of descs to disk. The method can be called from runFlushState()
+// only
+func (s *Scanner) flushState() error {
+	d := s.descs.Load().(descs)
+	return s.saveState(d)
+}
+
+func (s *Scanner) wait(ticker *time.Ticker) bool {
+	select {
+	case <-s.ctx.Done():
+		return false
+	case <-ticker.C:
+		return true
+	}
+}
+
+func (s *Scanner) loadState() (descs, error) {
+	s.logger.Info("Loading state from ", s.stStorage)
+
+	res := newDescs()
+	rb, err := s.stStorage.ReadData()
+	if err != nil {
+		if !os.IsNotExist(err) {
+			s.logger.Warn("No status found err=", err, " stStorage=", s.stStorage)
+			return res, err
+		}
+		return res, nil
+	}
+
+	if err = json.Unmarshal(rb, &res); err != nil {
+		return nil, fmt.Errorf("cannot unmarshal state from %v; cause: %v", s.stStorage, err)
+	}
+
+	return res, nil
+}
+
+func (s *Scanner) saveState(dscs descs) error {
+	data, err := json.Marshal(dscs)
+	if err != nil {
+		return fmt.Errorf("cannot marshal state=%v; cause: %v", dscs, err)
+	}
+
+	return s.stStorage.WriteData(data)
+}
+
+// getDescs scans paths and creates map of file desc(s) for all files matched in
+// the paths
+func (s *Scanner) getDescsToScan() descs {
+	paths := s.cfg.ScanPaths
+
+	res := make(descs)
+	files := s.getFilesToScan(paths)
+	for _, f := range files {
+		info, err := os.Stat(f)
+		if err != nil {
+			s.logger.Warn("Unable to get info for file=", f, ", skipping; cause: ", err)
+			continue
+		}
+
+		id := util.GetFileId(f, info)
+		res[id] = &desc{Id: id, File: f, ScanSize: info.Size(), Offset: 0}
+	}
+
+	if len(s.cfg.Exclude) > 0 {
+		exclds := []string{}
+		for _, ex := range s.cfg.Exclude {
+			// ignoring error, cause we already tried it when checked the config
+			re, _ := regexp.Compile(ex)
+			for id, d := range res {
+				if re.Match(util.StringToByteArray(d.File)) {
+					delete(res, id)
+					exclds = append(exclds, d.File)
+				}
+			}
+		}
+		s.excludes.Store(exclds)
+	}
+	return res
+}
+
+// replaceOrRotate iterates over new descriptors dssNew, which have been just
+// found and tries to apply old state for them if it exists. If there is old
+// state, but the file seems truncated, the offset will be reset.
+// returns number of changes were made.
+func (s *Scanner) replaceOrRotate(dssOld, dssNew descs) int {
+	res := 0
+	for id, ds := range dssNew {
+		dsOld, ok := dssOld[id]
+		if !ok {
+			s.logger.Info("New descriptor is found ", ds)
+			res++
+			continue
+		}
+
+		if dsOld.ScanSize <= ds.ScanSize && dsOld.getOffset() <= ds.ScanSize {
+			dssNew[id] = dsOld
+			continue
+		}
+
+		res++
+		s.logger.Info("Found descriptors inconsistency: old=", dsOld, " and the new one is ", ds, ". Seems like a rotation happens")
+	}
+
+	for id, dsOld := range dssOld {
+		if _, ok := dssNew[id]; !ok {
+			s.logger.Info("Seems like the file ", dsOld, " does not exist anymore. Forget about it.")
+			res++
+		}
+	}
+	return res
+}
+
+// getFilesToScan list of paths in GLOB (pathname patterns) style and turn
+// them to real file doing the following actions:
+// 1. Checks whether the file is readable (Stat could be taken)
+// 2. Skip directories
+func (s *Scanner) getFilesToScan(paths []string) []string {
+	ep := util.ExpandPaths(paths)
+	ff := make([]string, 0, len(ep))
+	for _, p := range ep {
+		var err error
+		fin, err := os.Stat(p)
+		if err != nil {
+			s.logger.Warn("Skipping path=", p, "; cause: ", err)
+			continue
+		}
+
+		if fin.IsDir() {
+			s.logger.Warn("Skipping path=", p, "; cause: the path is directory")
+			continue
+		}
+
+		ff = append(ff, p)
+	}
+	return util.RemoveDups(ff)
+}
+
+//=== descs
+func newDescs() descs {
+	return map[string]*desc{}
+}
+
+func (ds descs) MarshalJSON() ([]byte, error) {
+	dl := make([]*desc, 0, len(ds))
+	for _, d := range ds {
+		dl = append(dl, d)
+	}
+	return json.Marshal(&dl)
+}
+
+func (ds descs) UnmarshalJSON(data []byte) error {
+	dl := make([]*desc, 0, 5)
+	err := json.Unmarshal(data, &dl)
+	if err == nil {
+		for _, d := range dl {
+			ds[d.Id] = d
+		}
+	}
+	return err
+}
+
+func (ds descs) String() string {
+	return util.ToJsonStr(ds)
+}
+
+//=== desc
+
+func (d *desc) MarshalJSON() ([]byte, error) {
+	type alias desc
+	return json.Marshal(&struct {
+		*alias
+		Offset int64 `json:"offset"`
+	}{
+		alias:  (*alias)(d),
+		Offset: d.getOffset(),
+	})
+}
+
+func (d *desc) addOffset(val int64) {
+	atomic.AddInt64(&d.Offset, val)
+}
+
+func (d *desc) setOffset(val int64) {
+	atomic.StoreInt64(&d.Offset, val)
+}
+
+func (d *desc) getOffset() int64 {
+	return atomic.LoadInt64(&d.Offset)
+}
+
+func (d *desc) String() string {
+	return util.ToJsonStr(d)
+}
+
+//=== helpers
+func checkCfg(cfg *Config) error {
+	if cfg == nil {
+		return fmt.Errorf("invalid config=%v", cfg)
+	}
+	if cfg.EventMaxRecords <= 0 {
+		return fmt.Errorf("invalid config; eventMaxRecords=%v, must be > 0", cfg.EventMaxRecords)
+	}
+	if cfg.EventSendIntervalMs <= 100 {
+		return fmt.Errorf("invalid config; eventSendIntervalMs=%v, must be > 100ms", cfg.EventSendIntervalMs)
+	}
+	if cfg.ScanPathsIntervalSec <= 0 {
+		return fmt.Errorf("invalid config; scanPathsIntervalSec=%v, must be > 0sec", cfg.ScanPathsIntervalSec)
+	}
+	if cfg.StateFlushIntervalSec <= 0 {
+		return fmt.Errorf("invalid config; stateFlushIntervalSec=%v, must be > 0sec", cfg.StateFlushIntervalSec)
+	}
+	if cfg.RecordMaxSizeBytes < 64 || cfg.RecordMaxSizeBytes > 32*1024 {
+		return fmt.Errorf("invalid config; recordSizeMaxBytes=%v, must be in range [%v..%v]",
+			cfg.RecordMaxSizeBytes, 64, 32*1024)
+	}
+	for _, f := range cfg.FileFormats {
+		if err := checkFileFormat(f); err != nil {
+			return fmt.Errorf("invalid config; invalid fileFormat=%v, %v", f, err)
+		}
+	}
+
+	for _, ex := range cfg.Exclude {
+		if _, err := regexp.Compile(ex); err != nil {
+			return fmt.Errorf("invalid config; Could not compile regular expression in exclude: %s, err=%v", ex, err)
+		}
+	}
+	return nil
+}
+
+func checkFileFormat(f *FileFormat) error {
+	if strings.TrimSpace(f.PathMatcher) == "" {
+		return errors.New("patchMatcher must be non-empty")
+	}
+
+	_, err := syntax.Parse(f.PathMatcher, syntax.Perl)
+	if err != nil {
+		return fmt.Errorf("pathMatcher is invalid; %v", err)
+	}
+
+	return nil
+}
+
+// getTimeFormats returns slice of Date-time formats found in between ff - FileFormat
+// slice by matching filename with FileFormat regexps.
+func getTimeFormats(d *desc, ff []*FileFormat) []string {
+	fmts := []string{}
+	for _, f := range ff {
+		if match, _ := regexp.MatchString(f.PathMatcher, d.File); match {
+			fmts = append(fmts, f.TimeFormats...)
+		}
+	}
+	return fmts
+}
+
+// getDataFormat returns corresponding DataFromat (cTxtDataFormat, cJsonDataFormat etc.)
+// by matching d.File with different FileFormats provided in ff slice
+func getDataFormat(d *desc, ff []*FileFormat) string {
+	for _, f := range ff {
+		if match, _ := regexp.MatchString(f.PathMatcher, d.File); match {
+			return f.DataFormat
+		}
+	}
+	return cTxtDataFormat
+}
diff --git a/pkg/collector/scanner/scanner_test.go b/pkg/collector/scanner/scanner_test.go
new file mode 100644
index 0000000..d908c44
--- /dev/null
+++ b/pkg/collector/scanner/scanner_test.go
@@ -0,0 +1,172 @@
+package scanner
+
+import (
+	"bytes"
+	"crypto/md5"
+	"fmt"
+	"github.com/logrange/logrange/pkg/collector/model"
+	"io"
+	"os"
+	"path"
+	"path/filepath"
+	"runtime"
+	"testing"
+	"time"
+
+	"github.com/jrivets/log4g"
+)
+
+const (
+	testOutDir = "/tmp/scannertest"
+)
+
+var (
+	inOutMap = map[string]string{}
+	outFiles = map[string]*os.File{}
+	log      = log4g.GetLogger("TestIntegration")
+)
+
+func TestIntegration(t *testing.T) {
+	os.RemoveAll(testOutDir)
+	os.MkdirAll(testOutDir, 0777)
+
+	lg := log4g.GetLogger("collector.scanner")
+	log4g.SetLogLevel(lg.GetName(), log4g.DEBUG)
+
+	cfg := NewDefaultConfig()
+	cfg.ScanPathsIntervalSec = 5
+
+	_, d, _, _ := runtime.Caller(0)
+	cfg.ScanPaths = []string{
+		filepath.Dir(d) + "/testdata/logs/ubuntu/var/log/*.log",
+		filepath.Dir(d) + "/testdata/logs/ubuntu/var/log/*/*.log",
+	}
+
+	cfg.FileFormats = append(cfg.FileFormats, &FileFormat{
+		PathMatcher: ".*",
+		DataFormat:  cTxtDataFormat,
+		TimeFormats: []string{"DD/MMM/YYYY:HH:mm:ss ZZZZ"},
+	})
+
+	cl, err := NewScanner(cfg, NewInMemStateStorage())
+	if err != nil {
+		t.Fatal(err)
+	}
+	err = cl.Start()
+	if err != nil {
+		t.Fatal(err)
+	}
+
+	for {
+		select {
+		case ev := <-cl.Events():
+			if err := handle(ev); err != nil {
+				t.Fatal(err)
+			}
+		case <-time.After(time.Second * 5):
+			if err := cl.Stop(); err != nil {
+				t.Error(err)
+			}
+
+			closeOutFiles()
+			if len(inOutMap) == 0 {
+				log.Error("nothing was sent, nothing to compare!")
+				t.FailNow()
+			}
+			for src, dst := range inOutMap {
+				log.Info("comparing src=", src, ", dst=", dst)
+				eq, err := md5Eq(src, dst)
+				if err != nil || !eq {
+					log.Error("failed to compare src=", src, ", dst=", dst, "; err=", err, ", eq=", eq)
+					t.Fail()
+				}
+			}
+			return
+		}
+	}
+}
+
+func handle(ev *model.Event) error {
+	tFile := fmt.Sprintf("%v/%v", testOutDir, path.Base(ev.File))
+	for _, r := range ev.Records {
+		if err := appendToFile(tFile, r); err != nil {
+			return err
+		}
+	}
+
+	ev.Confirm()
+	inOutMap[ev.File] = tFile
+	return nil
+}
+
+func appendToFile(file string, r *model.Record) error {
+	fd, err := getOutFile(file)
+	if err != nil {
+		return err
+	}
+
+	/*	bb := bytes.Buffer{}
+		bb.Write([]byte{'\n', '>', 't', 's', '='})
+		bb.Write([]byte((*r.GetTs()).Format(time.RFC3339Nano)))
+		bb.Write([]byte{'|'})
+		bb.Write(r.Data)
+		bb.Write([]byte{'<'})*/
+	_, err = fd.Write(r.Data)
+	return err
+}
+
+func getOutFile(file string) (*os.File, error) {
+	if f, ok := outFiles[file]; ok {
+		return f, nil
+	}
+
+	fd, err := os.OpenFile(file, os.O_CREATE|os.O_WRONLY, 0640)
+	if err != nil {
+		return nil, err
+	}
+
+	outFiles[file] = fd
+	return fd, nil
+}
+
+func closeOutFiles() {
+	for _, fd := range outFiles {
+		fd.Sync()
+		fd.Close()
+	}
+}
+
+func md5Eq(f1, f2 string) (bool, error) {
+	f1h, err := md5File(f1)
+	log.Info("file ", f1, " hash=", f1h)
+	if err != nil {
+		return false, err
+	}
+
+	f2h, err := md5File(f2)
+	log.Info("file ", f2, " hash=", f2h)
+	if err != nil {
+		return false, err
+	}
+
+	return bytes.Equal(f1h, f2h), nil
+}
+
+func md5File(file string) ([]byte, error) {
+	fi, _ := os.Stat(file)
+	log.Info("file=", file, " size=", fi.Size())
+
+	f, err := os.Open(file)
+	if err != nil {
+		return nil, err
+	}
+
+	defer f.Close()
+	hash := md5.New()
+	if _, err := io.Copy(hash, f); err != nil {
+		return nil, err
+	}
+
+	var result []byte
+	return hash.Sum(result), err
+}
diff --git a/pkg/collector/scanner/status_storage.go b/pkg/collector/scanner/status_storage.go
new file mode 100644
index 0000000..a97b403
--- /dev/null
+++ b/pkg/collector/scanner/status_storage.go
@@ -0,0 +1,59 @@
+package scanner
+
+import (
+	"github.com/logrange/logrange/pkg/util"
+	"io/ioutil"
+	"os"
+)
+
+type (
+	// StateStorage interface allows to read and write serialized data of scanner status
+	StateStorage interface {
+		// ReadData will return data buffer. If the status is not
+		// found os.IsNotExist(err) will be true.
+		ReadData() ([]byte, error)
+		WriteData(buf []byte) error
+	}
+
+	// inmemStateStorage struct an in-mem StateStorage implementation
+	inmemStateStorage struct {
+		buf []byte
+	}
+
+	// fileStateStorage stuct a file StateStorage implementation
+	fileStateStorage struct {
+		fileName string
+	}
+)
+
+func NewInMemStateStorage() StateStorage {
+	return &inmemStateStorage{}
+}
+
+func NewFileStateStorage(fn string) StateStorage {
+	return &fileStateStorage{fn}
+}
+
+func (iss *inmemStateStorage) ReadData() ([]byte, error) {
+	if iss.buf == nil {
+		return nil, os.ErrNotExist
+	}
+	return iss.buf, nil
+}
+
+func (iss *inmemStateStorage) WriteData(buf []byte) error {
+	if buf == nil {
+		iss.buf = nil
+		return nil
+	}
+	iss.buf = util.BytesCopy(buf)
+	return nil
+}
+
+func (fss *fileStateStorage) ReadData() ([]byte, error) {
+	return ioutil.ReadFile(fss.fileName)
+}
+
+func (fss *fileStateStorage) WriteData(buf []byte) error {
+	return ioutil.WriteFile(fss.fileName, buf, 0640)
+}
diff --git a/pkg/collector/scanner/string_parser.go b/pkg/collector/scanner/string_parser.go
new file mode 100644
index 0000000..22c7c7b
--- /dev/null
+++ b/pkg/collector/scanner/string_parser.go
@@ -0,0 +1,158 @@
+package scanner
+
+import (
+	"fmt"
+	"github.com/logrange/logrange/pkg/collector/model"
+	"github.com/logrange/logrange/pkg/collector/scanner/dtparser"
+	"io"
+	"time"
+)
+
+type (
+	// string_parser provides every non-empty string, so it  returns Record for
+	// every input string
+	stringParser struct {
+		parser    *dtparser.Parser
+		lastTS    time.Time
+		fmtParser *dtparser.ParserFormat
+		stats     *linePStat
+
+		// maxSkip defines maximum value of the lines must be skipped.
+		// if it is -1, then any line should not be attempted to find a format,
+		// but current timestamp will be used
+		maxSkip int
+		skipped int
+		state   int
+	}
+
+	// line parser statistics
+	linePStat struct {
+		hits map[string]int64
+	}
+)
+
+const (
+	slpStateNoFormat = iota
+	slpStateAdjusting
+	slpStateSkipping
+)
+
+// new_string_parser creates new string_parser, expects custom
+// dtFormats in form "MMM D, YYYY h:mm:ss P" (see dtparser package) etc. if any.
+func newStringParser(dtFormats ...string) *stringParser {
+	slp := new(stringParser)
+
+	if len(dtFormats) > 0 {
+		dtFmts := make([]string, len(dtparser.KnownFormats)+len(dtFormats))
+		copy(dtFmts[:len(dtFormats)], dtFormats)
+		copy(dtFmts[len(dtFormats):], dtparser.KnownFormats)
+		slp.parser = dtparser.NewParser(dtFmts)
+	} else {
+		slp.parser = dtparser.NewDefaultParser()
+	}
+
+	slp.stats = newLinePStat()
+	slp.maxSkip = 10
+	slp.state = slpStateAdjusting
+	return slp
+}
+
+func (slp *stringParser) apply(line []byte) *model.Record {
+	rec := model.NewEmptyRecord()
+	rec.Data = line
+	slp.applyTimestamp(line, rec)
+	return rec
+}
+
+func (slp *stringParser) applyTimestamp(line []byte, rec *model.Record) {
+	var (
+		err error
+		tm  time.Time
+	)
+
+	fp := slp.fmtParser
+	switch slp.state {
+	case slpStateNoFormat:
+		slp.lastTS = time.Now()
+	case slpStateAdjusting:
+		if fp != nil {
+			tm, err = fp.Parse(line)
+			if err == nil {
+				slp.lastTS = tm
+				break
+			}
+		}
+
+		tm, fp = slp.parser.Parse(line, fp)
+		if fp != nil {
+			slp.maxSkip = 1
+			slp.skipped = 0
+			slp.fmtParser = fp
+			slp.lastTS = tm
+			break
+		}
+
+		slp.fmtParser = nil
+		slp.skipped++
+		if slp.skipped > slp.maxSkip {
+			slp.state = slpStateSkipping
+			slp.skipped = 0
+		}
+
+	case slpStateSkipping:
+		slp.skipped++
+		if slp.skipped > slp.maxSkip {
+			slp.state = slpStateAdjusting
+			if slp.maxSkip < 128 {
+				slp.maxSkip <<= 1
+			}
+			slp.skipped = slp.maxSkip
+		}
+	}
+
+	slp.hitStats(fp)
+	rec.SetTs(slp.lastTS)
+}
+
+func (slp *stringParser) hitStats(pf *dtparser.ParserFormat) {
+	if slp.stats == nil {
+		return
+	}
+	slp.stats.hit(pf)
+}
+
+func newLinePStat() *linePStat {
+	lps := new(linePStat)
+	lps.hits = make(map[string]int64)
+	return lps
+}
+
+func (lps *linePStat) hit(pf *dtparser.ParserFormat) {
+	name := "unknown"
+	if pf != nil {
+		name = pf.GetFormat()
+	}
+	lps.hits[name]++
+}
+
+func (lps *linePStat) PrintStatus(w io.Writer) {
+	total := int64(0)
+	for _, v := range lps.hits {
+		total += v
+	}
+
+	fmt.Fprintf(w, "--- Found formats\n")
+	for k, v := range lps.hits {
+		perc := float32(100*v) / float32(total)
+		fmt.Fprintf(w, "\"%s\"\t%d hit(s)(%5.2f%%) \n", k, v, perc)
+	}
+
+	unknw := lps.hits["unknown"]
+	undidx := float32(100.0)
+	if total > 0 {
+		undidx = 100.0 - (float32(unknw) * 100.0 / float32(total))
+	}
+
+	fmt.Fprintf(w, "total: %d records consiered, successIdx %5.2f%%\n",
+		total, undidx)
+}
diff --git a/pkg/collector/scanner/string_parser_test.go b/pkg/collector/scanner/string_parser_test.go
new file mode 100644
index 0000000..485894e
--- /dev/null
+++ b/pkg/collector/scanner/string_parser_test.go
@@ -0,0 +1,61 @@
+package scanner
+
+import (
+	"context"
+	"github.com/logrange/logrange/pkg/util"
+	"os"
+	"path/filepath"
+	"runtime"
+
+	"github.com/jrivets/log4g"
+	"testing"
+)
+
+func TestStringParser(t *testing.T) {
+	logger := log4g.GetLogger("TestStringParser")
+	_, d, _, _ := runtime.Caller(0)
+	fTmps := []string{
+		filepath.Dir(d) + "/testdata/logs/ubuntu/var/log/wifi*.log",
+		filepath.Dir(d)+ "/testdata/logs/ubuntu/var/log/*/*.log",
+	}
+
+	files := getFilePaths(fTmps)
+	logger.Info("files=", files)
+
+	for _, fn := range files {
+		f, err := os.Open(fn)
+		if err != nil {
+			logger.Warn("Could not open file ", fn, ", err=", err)
+		}
+		lr := newLineReader(f, 64000, context.Background())
+		lp := newStringParser()
+		for err == nil {
+			var ln []byte
+			ln, err = lr.readLine()
+			if err == nil {
+				lp.apply(ln)
+			}
+		}
+		f.Close()
+		logger.Info("File read stats: \nfilename=", fn, lp.stats)
+	}
+}
+
+func getFilePaths(paths []string) []string {
+	ep := util.ExpandPaths(paths)
+	ff := make([]string, 0, len(ep))
+	for _, p := range ep {
+		if _, err := filepath.EvalSymlinks(p); err != nil {
+			continue
+		}
+		fin, err := os.Stat(p)
+		if err != nil {
+			continue
+		}
+		if fin.IsDir() {
+			continue
+		}
+		ff = append(ff, p)
+	}
+	return util.RemoveDups(ff)
+}
diff --git a/pkg/collector/scanner/worker.go b/pkg/collector/scanner/worker.go
new file mode 100644
index 0000000..4c32c25
--- /dev/null
+++ b/pkg/collector/scanner/worker.go
@@ -0,0 +1,153 @@
+package scanner
+
+import (
+	"context"
+	"github.com/logrange/logrange/pkg/collector/model"
+	"io"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/jrivets/log4g"
+)
+
+type (
+	// workerConfig is a structure which is used to provide init parameters for a
+	// a new worker
+	workerConfig struct {
+		desc   *desc
+		parser Parser
+		logger log4g.Logger
+		ctx    context.Context
+		cancel context.CancelFunc
+
+		// EventMaxRecCount defines maximum number of records per event
+		EventMaxRecCount int
+	}
+
+	worker struct {
+		logger      log4g.Logger
+		desc        *desc
+		maxRecCount int
+		ctx         context.Context
+		ctxCancel   context.CancelFunc
+		recParser   Parser
+		lock        sync.Mutex
+		confCh      chan struct{}
+		existChck   time.Time
+		state       int32
+	}
+
+	WorkerStats struct {
+		Filename    string
+		Id          string
+		ParserStats *ParserStats
+	}
+
+	onEventFunc func(ev *model.Event) error
+)
+
+const (
+	wsRunning = int32(iota)
+	wsRunningUntilEof
+	wsClosed
+)
+
+func newWorker(wc *workerConfig) *worker {
+	w := new(worker)
+	w.logger = wc.logger
+	w.desc = wc.desc
+	w.ctx, w.ctxCancel = wc.ctx, wc.cancel
+	w.recParser = wc.parser
+	w.maxRecCount = wc.EventMaxRecCount
+	w.state = wsRunning
+	w.logger.Info("New worker for ", w.desc)
+	return w
+}
+
+func (w *worker) run(oef onEventFunc) (err error) {
+	if err = w.recParser.SetStreamPos(w.desc.getOffset()); err != nil {
+		return err
+	}
+	w.confCh = make(chan struct{})
+	defer close(w.confCh)
+
+	recs := make([]*model.Record, 0, w.maxRecCount)
+	for w.ctx.Err() == nil && err == nil {
+		var rec *model.Record
+		rec, err = w.recParser.NextRecord()
+		if rec != nil {
+			recs = append(recs, rec)
+		}
+
+		if err == io.EOF || len(recs) == w.maxRecCount {
+			eof := err == io.EOF
+			err = w.sendRecs(recs, oef, w.recParser.GetStreamPos())
+			recs = recs[:0]
+			if eof && atomic.LoadInt32(&w.state) == wsRunningUntilEof && err == nil {
+				// break the cycle
+				w.logger.Info("EOF is reached in wsRunningUntilEof state")
+				err = io.EOF
+			}
+		}
+	}
+	w.logger.Info("Over handling records err=", err)
+	return err
+}
+
+func (w *worker) sendRecs(recs []*model.Record, oef onEventFunc, offs int64) error {
+	if len(recs) == 0 {
+		select {
+		case <-w.ctx.Done():
+		case <-time.After(time.Second):
+		}
+		return nil
+	}
+
+	defer w.clear(recs)
+	ev := model.NewEvent(w.desc.File, recs, w.confCh)
+	err := oef(ev)
+	if err != nil {
+		return err
+	}
+
+	select {
+	case <-w.ctx.Done():
+	case w.confCh <- struct{}{}:
+		w.desc.setOffset(offs)
+	}
+	return nil
+}
+
+func (w *worker) clear(recs []*model.Record) {
+	for i := 0; i < len(recs); i++ {
+		recs[i] = nil
+	}
+}
+
+func (w *worker) stopWhenEof() {
+	w.logger.Info("Set wsRunningUntilEof")
+	atomic.CompareAndSwapInt32(&w.state, wsRunning, wsRunningUntilEof)
+}
+
+func (w *worker) isClosed() bool {
+	return atomic.LoadInt32(&w.state) == wsClosed
+}
+
+func (w *worker) Close() (err error) {
+	w.lock.Lock()
+	defer w.lock.Unlock()
+
+	atomic.StoreInt32(&w.state, wsClosed)
+	w.ctxCancel()
+	if w.recParser != nil {
+		err = w.recParser.Close()
+	}
+	return
+}
+
+func (w *worker) GetStats() *WorkerStats {
+	ps := w.recParser.GetStats()
+	ps.Pos = w.desc.getOffset()
+	return &WorkerStats{Filename: w.desc.File, Id: w.desc.Id, ParserStats: ps}
+}
diff --git a/pkg/collector/status_updater.go b/pkg/collector/status_updater.go
new file mode 100644
index 0000000..60bfdca
--- /dev/null
+++ b/pkg/collector/status_updater.go
@@ -0,0 +1,161 @@
+package collector
+
+import (
+	"bytes"
+	"context"
+	"encoding/json"
+	"fmt"
+	"github.com/logrange/logrange/pkg/collector/ingestor"
+	"github.com/logrange/logrange/pkg/collector/scanner"
+	"github.com/logrange/logrange/pkg/util"
+	"io/ioutil"
+	"math"
+	"os"
+	"text/tabwriter"
+	"time"
+
+	"github.com/jrivets/log4g"
+)
+
+type (
+	// StatusFileUpdater struct allows to collect information from scanner, ingestor, and config.
+	StatusFileUpdater struct {
+		cfg    *Config
+		scn    *scanner.Scanner
+		ing    *ingestor.Ingestor
+		logger log4g.Logger
+	}
+)
+
+const (
+	cStatFileUpdateSec = 5 * time.Second
+)
+
+func NewStatusFileUpdater(cfg *Config, scn *scanner.Scanner, ing *ingestor.Ingestor) *StatusFileUpdater {
+	sfu := new(StatusFileUpdater)
+	sfu.cfg = cfg
+	sfu.scn = scn
+	sfu.ing = ing
+	sfu.logger = log4g.GetLogger("collector.ingestor.statusFileUpdater")
+	return sfu
+}
+
+func (sfu *StatusFileUpdater) PrintStatusFile() {
+	res, err := ioutil.ReadFile(sfu.cfg.StatusFile)
+	if err != nil {
+		fmt.Println("ERROR: Seems no agent running.")
+		return
+	}
+	fmt.Println(string(res))
+}
+
+func (sfu *StatusFileUpdater) Run(ctx context.Context) {
+	if sfu.cfg.StatusFile == "" {
+		sfu.logger.Warn("Could not run status file update, file is not set up.")
+		return
+	}
+
+	go func() {
+		sfu.logger.Info("Will update status every ", cStatFileUpdateSec, " to ", sfu.cfg.StatusFile)
+		defer os.Remove(sfu.cfg.StatusFile)
+		for {
+			sfu.saveStatFile()
+			select {
+			case <-ctx.Done():
+				sfu.logger.Info("Stop writing stat file")
+				return
+			case <-time.After(cStatFileUpdateSec):
+			}
+		}
+	}()
+}
+
+func (sfu *StatusFileUpdater) saveStatFile() {
+	var w bytes.Buffer
+	tw := new(tabwriter.Writer)
+	tw.Init(&w, 0, 8, 1, ' ', 0)
+
+	fmt.Fprintf(tw, "*********************\n")
+	fmt.Fprintf(tw, "* Logrange Collector \n")
+	fmt.Fprintf(tw, "*********************\n")
+	fmt.Fprintf(tw, "\n=== Configuration file ===\n")
+	buf, _ := json.MarshalIndent(sfu.cfg, "", "   ")
+	fmt.Fprintf(tw, string(buf))
+	fmt.Fprintf(tw, "\n\nStatus at %s\n\n", time.Now().String())
+	fmt.Fprintf(tw, "=== Connection ===\n")
+	fmt.Fprintf(tw, "\tAggregator:\t%s\n", sfu.cfg.Ingestor.Server)
+	fmt.Fprintf(tw, "\tRecs per packet:\t%d\n", sfu.cfg.Ingestor.PacketMaxRecords)
+	fmt.Fprintf(tw, "\tHeartbeat:\t%dms\n", sfu.cfg.Ingestor.HeartBeatMs)
+
+	if sfu.ing.IsConnected() {
+		fmt.Fprintf(tw, "\tStatus:\tCONNECTED\n")
+	} else {
+		fmt.Fprintf(tw, "\tStatus:\tCONNECTING...\n")
+	}
+
+	fmt.Fprintf(tw, "\n=== Scanner ===\n")
+
+	gs := sfu.scn.GetStats()
+	fmt.Fprintf(tw, "\tscan paths:\t%v\n", gs.Config.ScanPaths)
+	fmt.Fprintf(tw, "\tscan intervals:\tevery %d sec.\n", gs.Config.ScanPathsIntervalSec)
+	fmt.Fprintf(tw, "\tstate file:\t%s\n", sfu.cfg.StateFile)
+	fmt.Fprintf(tw, "\tstate update:\tevery %d sec.\n", gs.Config.StateFlushIntervalSec)
+	fmt.Fprintf(tw, "\tfile formats:\t%v\n", gs.Config.FileFormats)
+	fmt.Fprintf(tw, "\trecord max size:\t%s\n", util.FormatSize(int64(gs.Config.RecordMaxSizeBytes)))
+	fmt.Fprintf(tw, "\trecords per pack:\t%d\n\n", gs.Config.EventMaxRecords)
+	fmt.Fprintf(tw, "--- scanned files (%d) ---\n", len(gs.Workers))
+	for _, wkr := range gs.Workers {
+		fmt.Fprintf(tw, "\t%s\n", wkr.Filename)
+	}
+
+	fmt.Fprintf(tw, "\n--- excluded files (%d) ---\n", len(gs.Excludes))
+	for _, ef := range gs.Excludes {
+		fmt.Fprintf(tw, "\t%s\n", ef)
+	}
+
+	knwnTags := sfu.ing.GetKnownTags()
+	totalPerc := float64(0)
+	for i, wkr := range gs.Workers {
+		fmt.Fprintf(tw, "\n--- Scanner %d\n", i+1)
+		fmt.Fprintf(tw, "\tfile:\t%s\n", wkr.Filename)
+		fmt.Fprintf(tw, "\tdesc-id:\t%s\n", wkr.Id)
+		fmt.Fprintf(tw, "\tdata-type:\t%s\n", wkr.ParserStats.DataType)
+		size := wkr.ParserStats.Size
+		fmt.Fprintf(tw, "\tsize:\t%s\n", util.FormatSize(size))
+
+		pos := wkr.ParserStats.Pos
+		perc := float64(100)
+		if size > 0 {
+			perc = float64(pos) * perc / float64(size)
+		}
+		perc = math.Max(0.0, math.Min(100.0, perc))
+		totalPerc += perc
+
+		if tags, ok := knwnTags[wkr.Filename]; ok {
+			fmt.Fprintf(tw, "\tknwnTags: \n\t%s", tags)
+		} else {
+			fmt.Fprintf(tw, "\tknwnTags:\t<data is not sent yet, or no new data for 5 mins>\n")
+		}
+
+		fmt.Fprintf(tw, "\tprogress:\t%s %s\n", util.FormatSize(pos), util.FormatProgress(30, perc))
+		if len(wkr.ParserStats.DateFormats) > 0 {
+			fmt.Fprintf(tw, "\n\tFormats:\n")
+			tot := int64(0)
+			for _, v := range wkr.ParserStats.DateFormats {
+				tot += v
+			}
+			for dtf, v := range wkr.ParserStats.DateFormats {
+				perc := float64(v) * 100.0 / float64(tot)
+				fmt.Fprintf(tw, "\t\t\"%s\"\t%5.2f%%(%d of %d records have the format)\n", dtf, perc, v, tot)
+			}
+		}
+		fmt.Fprintf(tw, "-----------\n")
+	}
+	if len(gs.Workers) > 0 {
+		totalPerc /= float64(len(gs.Workers))
+	}
+	fmt.Fprintf(tw, "\nReplica status: %s\n", util.FormatProgress(40, totalPerc))
+
+	tw.Flush()
+	ioutil.WriteFile(sfu.cfg.StatusFile, []byte(w.Bytes()), 0644)
+}
diff --git a/pkg/collection/lru.go b/pkg/dstruct/lru.go
similarity index 99%
rename from pkg/collection/lru.go
rename to pkg/dstruct/lru.go
index 2a08a5b..3068729 100644
--- a/pkg/collection/lru.go
+++ b/pkg/dstruct/lru.go
@@ -1,4 +1,4 @@
-package collection
+package dstruct
 
 import (
 	"time"
diff --git a/pkg/collection/lru_test.go b/pkg/dstruct/lru_test.go
similarity index 99%
rename from pkg/collection/lru_test.go
rename to pkg/dstruct/lru_test.go
index a495304..84756df 100644
--- a/pkg/collection/lru_test.go
+++ b/pkg/dstruct/lru_test.go
@@ -1,4 +1,4 @@
-package collection
+package dstruct
 
 import (
 	"math/rand"
diff --git a/pkg/dstruct/ring_buffer.go b/pkg/dstruct/ring_buffer.go
new file mode 100644
index 0000000..806ca1a
--- /dev/null
+++ b/pkg/dstruct/ring_buffer.go
@@ -0,0 +1,127 @@
+package dstruct
+
+type (
+	// RingBuffer - the ring buffer with fixed capacity. The container has
+	// head and tail. It provides operations that allows to manipulate the
+	// data stored in the buffer.
+	//
+	// Handle with caution! The buffer doesn't free elements, and doesn't nilling
+	// them intentionally. It was made with an intention to minimize memory allocations
+	// for the stored elements. It is the buffer's user responsibility to free
+	// and nillify stored values.
+	RingBuffer struct {
+		v []interface{}
+		h int
+		n int
+	}
+)
+
+// NewRingBuffer - returns new ring buffer with size elements reserved
+func NewRingBuffer(size int) *RingBuffer {
+	if size < 1 {
+		panic("size must be positive")
+	}
+	rb := new(RingBuffer)
+	rb.v = make([]interface{}, size, size)
+	return rb
+}
+
+// Head - returns head's element. Will panic if size of the RingBuffer is 0
+func (rb *RingBuffer) Head() interface{} {
+	if rb.n == 0 {
+		panic("Buffer is empty")
+	}
+	return rb.v[rb.h]
+}
+
+// Tail - returns tail's element. Will panic if size of the RingBuffer is 0
+func (rb *RingBuffer) Tail() interface{} {
+	if rb.n == 0 {
+		panic("Buffer is empty")
+	}
+	return rb.v[rb.getIdx(rb.h+rb.n-1)]
+}
+
+// At - returns element at the index i, countin from the head. Will panic if
+// the index is out of bounds
+func (rb *RingBuffer) At(i int) interface{} {
+	rb.checkIdx(i)
+	return rb.v[rb.getIdx(rb.h+i)]
+}
+
+// Set - assign value v for the element i, counting from the head.
+func (rb *RingBuffer) Set(i int, v interface{}) {
+	rb.checkIdx(i)
+	rb.v[rb.getIdx(rb.h+i)] = v
+}
+
+// Len - returns current buffer size
+func (rb *RingBuffer) Len() int {
+	return rb.n
+}
+
+// Capacity - returns the buffer capacity
+func (rb *RingBuffer) Capacity() int {
+	return len(rb.v)
+}
+
+// AdvanceTail - moves the tail and increases the current buffer size by 1.
+// if the buffer size reaches the maximum capacity, it will return head element,
+// moving the head and tail both to 1 position.
+func (rb *RingBuffer) AdvanceTail() interface{} {
+	if rb.n == len(rb.v) {
+		rb.h = rb.getIdx(rb.h + 1)
+	} else {
+		rb.n++
+	}
+	return rb.v[rb.getIdx(rb.h+rb.n-1)]
+}
+
+// AdvanceHead - advances head and reduce the buffer size to 1 (head) element.
+// It returns the element, which was at head, before the operation
+func (rb *RingBuffer) AdvanceHead() interface{} {
+	if rb.n < 1 {
+		panic("The buffer is empty")
+	}
+	rb.n--
+	v := rb.v[rb.h]
+	rb.h = rb.getIdx(rb.h + 1)
+	return v
+}
+
+// Push - places value v at the tail. It will increase the buffer size, or pops
+// head element if the buffer's capacity is reached. The previos element stored
+// at the new tail position is returned. After the operation tail points to the
+// new value v.
+func (rb *RingBuffer) Push(v interface{}) interface{} {
+	r := rb.AdvanceTail()
+	rb.v[rb.getIdx(rb.h+rb.n-1)] = v
+	return r
+}
+
+// IsFull - returns true if the buffer is full Len() == Capacity()
+func (rb *RingBuffer) IsFull() bool {
+	return rb.n == len(rb.v)
+}
+
+// Clear - drops the buffer size to 0
+func (rb *RingBuffer) Clear() {
+	rb.h = 0
+	rb.n = 0
+}
+
+func (rb *RingBuffer) getIdx(i int) int {
+	if i >= len(rb.v) {
+		return i - len(rb.v)
+	}
+	if i < 0 {
+		return len(rb.v) + i
+	}
+	return i
+}
+
+func (rb *RingBuffer) checkIdx(i int) {
+	if i < 0 || i >= rb.n {
+		panic("Index out of bounds")
+	}
+}
diff --git a/pkg/dstruct/ring_buffer_test.go b/pkg/dstruct/ring_buffer_test.go
new file mode 100644
index 0000000..7a4a899
--- /dev/null
+++ b/pkg/dstruct/ring_buffer_test.go
@@ -0,0 +1,107 @@
+package dstruct
+
+import (
+	"sync"
+	"testing"
+
+	"math/rand"
+	"time"
+)
+
+func TestGeneral(t *testing.T) {
+	r := NewRingBuffer(3)
+	if r.Len() != 0 || r.Capacity() != 3 {
+		t.Fatal("wrong constrains")
+	}
+	r.Push(int(1))
+	r.Push(int(2))
+	r.Push(int(3))
+	if r.Len() != 3 {
+		t.Fatal("wrong size, must be 3, but ", r.Len())
+	}
+
+	if r.AdvanceHead().(int) != 1 {
+		t.Fatal("Expecting 1")
+	}
+	r.Push(int(4))
+
+	if r.AdvanceTail().(int) != 2 {
+		t.Fatal("Expecting 2")
+	}
+
+	if r.At(0).(int) != 3 || r.At(1).(int) != 4 || r.At(2).(int) != 2 {
+		t.Fatal("Wrong values ", r.v)
+	}
+
+	r.Set(2, 5)
+	if r.Tail().(int) != 5 || r.At(2) != 5 {
+		t.Fatal("Wrong values ", r.v)
+	}
+}
+
+func TestPanicing(t *testing.T) {
+	if !catch(func() { NewRingBuffer(0) }) {
+		t.Fatal("Expecting panic - wrong size")
+	}
+
+	r := NewRingBuffer(5)
+	if !catch(func() { r.Head() }) {
+		t.Fatal("Expecting panic - head on 0 sized buf")
+	}
+
+	if !catch(func() { r.Tail() }) {
+		t.Fatal("Expecting panic - tail on 0 sized buf")
+	}
+
+	if !catch(func() { r.AdvanceHead() }) {
+		t.Fatal("Expecting panic - advance head on 0 sized buf")
+	}
+
+	if !catch(func() { r.At(0) }) {
+		t.Fatal("Expecting panic - at on 0 sized buf")
+	}
+
+	r.Push(1)
+	if catch(func() { r.At(0) }) || !catch(func() { r.At(1) }) {
+		t.Fatal("Expecting panic - index out of boundx")
+	}
+}
+
+func BenchmarkPush(b *testing.B) {
+	var m sync.Mutex
+	rand.Seed(time.Now().UnixNano())
+	rb := NewRingBuffer(12000)
+	b.ResetTimer()
+	for n := 0; n < b.N; n++ {
+		r := rand.Intn(100) + 1
+		for i := 0; i < r; i++ {
+			m.Lock()
+			rb.Push(i + r)
+			m.Unlock()
+		}
+	}
+}
+
+func BenchmarkPushNS(b *testing.B) {
+	rand.Seed(time.Now().UnixNano())
+	rb := NewRingBuffer(12000)
+	rb2 := NewRingBuffer(12000)
+	b.ResetTimer()
+	for n := 0; n < b.N; n++ {
+		r := rand.Intn(100) + 1
+		for i := 0; i < r; i++ {
+			rb.Push(i + r)
+		}
+		for i := 0; i < r; i++ {
+			rb2.Push(i + r)
+		}
+	}
+}
+
+func catch(f func()) (v bool) {
+	defer func() {
+		v = recover() != nil
+	}()
+	f()
+	return v
+}
diff --git a/pkg/dstruct/timeseries.go b/pkg/dstruct/timeseries.go
new file mode 100644
index 0000000..2b86afd
--- /dev/null
+++ b/pkg/dstruct/timeseries.go
@@ -0,0 +1,148 @@
+package dstruct
+
+import (
+	"fmt"
+	"time"
+)
+
+type (
+	// Timeseries is a structure, which keeps time-series in the number of chained
+	// buckets. This is a time-sliding window which can be used for smoothing
+	// out an observed scalar value.
+	Timeseries struct {
+		// tail points to the last bucket, which points to head one etc.
+		tail *ts_bucket
+
+		// clockNow is the clock function. It used to get the current time
+		clockNow TsClockNowF
+
+		// bktDur is the bucket size in time duration
+		bktDur time.Duration
+
+		// tsDur is the time-series size in time duration
+		tsDur time.Duration
+
+		// newVal function returns new counted value (see TsValue)
+		newVal TsNewValueF
+		total  TsValue
+	}
+
+	// TsNewValueF a function which constructs a TsValue
+	TsNewValueF func() TsValue
+
+	// TsClockNowF a function whic returns current time
+	TsClockNowF func() time.Time
+
+	// TsValue is an interface which represents an immutable scalar value
+	TsValue interface {
+		Add(val TsValue) TsValue
+		Sub(val TsValue) TsValue
+	}
+
+	// TsInt implements TsValue for int
+	TsInt int
+
+	ts_bucket struct {
+		next  *ts_bucket
+		sTime time.Time
+		val   TsValue
+	}
+)
+
+// NewTimeseries same as NewTimeseriesWithClock, but provides system time.Now()
+// for discovering current time.
+func NewTimeseries(bktDur, tsDur time.Duration, newValF TsNewValueF) *Timeseries {
+	return NewTimeseriesWithClock(bktDur, tsDur, newValF, time.Now)
+}
+
+// NewTimeseriesWithClock constructs new Timeseries value. Expects to receive
+// the bucket size(bktDur in time duration), the time-series in time duration
+// in the tsDur parameter, the newValF allows to create new scalar values and
+// the clck is a function which allows to discover current time
+func NewTimeseriesWithClock(bktDur, tsDur time.Duration, newValF TsNewValueF, clck TsClockNowF) *Timeseries {
+	if bktDur > tsDur || bktDur <= 0 {
+		panic(fmt.Sprint("Wrong durations: both timeseries duration=", tsDur, " and bucket one=", bktDur, " must be positive, and the first one should be bigger then second one."))
+	}
+	ts := new(Timeseries)
+	ts.tail = nil
+	ts.clockNow = clck
+	ts.bktDur = bktDur
+	ts.tsDur = tsDur
+	ts.newVal = newValF
+	ts.total = newValF()
+
+	ts.tail = new(ts_bucket)
+	ts.tail.next = ts.tail
+	ts.tail.val = newValF()
+	ts.tail.sTime = clck().Truncate(bktDur)
+	return ts
+}
+
+func (ts *Timeseries) Add(val TsValue) {
+	now := ts.sweep()
+	bkt := ts.getBucket(now)
+	bkt.val = bkt.val.Add(val)
+	ts.total = ts.total.Add(val)
+}
+
+func (ts *Timeseries) Total() TsValue {
+	ts.sweep()
+	return ts.total
+}
+
+func (ts *Timeseries) StartTime() time.Time {
+	return ts.tail.next.sTime
+}
+
+func (ts *Timeseries) sweep() time.Time {
+	now := ts.clockNow()
+	if now.Sub(ts.tail.sTime) >= ts.tsDur {
+		ts.total = ts.newVal()
+		ts.tail.next = ts.tail
+		ts.tail.val = ts.newVal()
+		ts.tail.sTime = now.Truncate(ts.bktDur)
+		return now
+	}
+
+	head := ts.tail.next
+	for head != ts.tail && now.Sub(head.sTime) >= ts.tsDur {
+		ts.total = ts.total.Sub(head.val)
+		h := head.next
+		head.next = nil
+		head.val = nil
+		head = h
+		ts.tail.next = head
+	}
+	return now
+}
+
+func (ts *Timeseries) getBucket(now time.Time) *ts_bucket {
+	d := now.Sub(ts.tail.sTime)
+	if d < 0 {
+		panic("don't support to add value in past")
+	}
+
+	if d < ts.bktDur {
+		return ts.tail
+	}
+
+	newTail := new(ts_bucket)
+	newTail.val = ts.newVal()
+	newTail.next = ts.tail.next
+	newTail.sTime = now.Truncate(ts.bktDur)
+	ts.tail.next = newTail
+	ts.tail = newTail
+	return newTail
+}
+
+func NewTsInt() TsValue {
+	return TsInt(0)
+}
+
+func (ti TsInt) Add(val TsValue) TsValue {
+	return ti + val.(TsInt)
+}
+
+func (ti TsInt) Sub(val TsValue) TsValue {
+	return ti - val.(TsInt)
+}
diff --git a/pkg/dstruct/timeseries_test.go b/pkg/dstruct/timeseries_test.go
new file mode 100644
index 0000000..524b78e
--- /dev/null
+++ b/pkg/dstruct/timeseries_test.go
@@ -0,0 +1,99 @@
+package dstruct
+
+import (
+	"testing"
+	"time"
+)
+
+func TestTsInt(t *testing.T) {
+	i := TsInt(10)
+	if i.Add(TsInt(20)) != TsInt(TsInt(30)) || i.Sub(TsInt(20)) != TsInt(-10) || i != TsInt(10) {
+		t.Fatal("Something goes wrong with Add or Sub for TsInt")
+	}
+}
+
+func TestTsIncremental(t *testing.T) {
+	bktSize := time.Second
+	tsSize := 3 * bktSize
+	st := time.Now()
+	clck := func() time.Time {
+		return st
+	}
+
+	ts := NewTimeseriesWithClock(bktSize, tsSize, NewTsInt, clck)
+	if ts.Total().(TsInt) != TsInt(0) {
+		t.Fatal("Should be 0!")
+	}
+	st = st.Add(time.Millisecond)
+
+	ts.Add(TsInt(1))
+	if ts.Total().(TsInt) != TsInt(1) || ts.tail.next != ts.tail {
+		t.Fatal("Should be 1!")
+	}
+	st = st.Add(time.Second)
+
+	ts.Add(TsInt(1))
+	if ts.Total().(TsInt) != TsInt(2) || ts.tail.next == ts.tail {
+		t.Fatal("Should be 2!")
+	}
+	st = st.Add(time.Second)
+
+	ts.Add(TsInt(1))
+	if ts.Total().(TsInt) != TsInt(3) {
+		t.Fatal("Should be 3!")
+	}
+	st = st.Add(time.Second)
+
+	ts.Add(TsInt(1))
+	if ts.Total().(TsInt) != TsInt(3) {
+		t.Fatal("Should be 3!")
+	}
+
+	st = st.Add(5 * time.Second)
+	if ts.Total().(TsInt) != TsInt(0) || ts.tail.next != ts.tail {
+		t.Fatal("Should be 0!")
+	}
+}
+
+func TestTsSameBucket(t *testing.T) {
+	bktSize := time.Second
+	tsSize := 2 * bktSize
+	st := time.Now()
+	clck := func() time.Time {
+		return st
+	}
+
+	ts := NewTimeseriesWithClock(bktSize, tsSize, NewTsInt, clck)
+	if ts.Total().(TsInt) != TsInt(0) {
+		t.Fatal("Should be 0!")
+	}
+	st = st.Add(10 * time.Millisecond)
+
+	ts.Add(TsInt(1))
+	ts.Add(TsInt(2))
+	ts.Add(TsInt(3))
+
+	if ts.Total().(TsInt) != TsInt(6) || ts.tail.next != ts.tail {
+		t.Fatal("Should be 6!")
+	}
+
+	st = st.Add(time.Second)
+	if ts.Total().(TsInt) != TsInt(6) || ts.tail.next != ts.tail {
+		t.Fatal("Should be 6!")
+	}
+
+	ts.Add(TsInt(-1))
+	if ts.Total().(TsInt) != TsInt(5) || ts.tail.next == ts.tail {
+		t.Fatal("Should be 5!")
+	}
+
+	st = st.Add(time.Second)
+	if ts.Total().(TsInt) != TsInt(-1) || ts.tail.next != ts.tail {
+		t.Fatal("Should be -1!")
+	}
+
+	st = st.Add(time.Second)
+	if ts.Total().(TsInt) != TsInt(0) || ts.tail.next != ts.tail {
+		t.Fatal("Should be 0!")
+	}
+}
diff --git a/pkg/logevent/log_event.go b/pkg/logevent/log_event.go
new file mode 100644
index 0000000..86b1805
--- /dev/null
+++ b/pkg/logevent/log_event.go
@@ -0,0 +1,126 @@
+package logevent
+
+import (
+	"fmt"
+)
+
+type (
+	LogEvent struct {
+		tgid int64
+		ts   int64
+		msg  WeakString
+		tgl  WeakString
+	}
+)
+
+func (le *LogEvent) Init(ts int64, msg WeakString) {
+	le.ts = ts
+	le.msg = msg
+}
+
+func (le *LogEvent) InitWithTagLine(ts int64, msg WeakString, tgl TagLine) {
+	le.ts = ts
+	le.msg = msg
+	le.tgl = WeakString(tgl)
+}
+
+// GetTimestamp returns timestamp in nanoseconds. It could be negative if
+// it less than 01/01/1970
+func (le *LogEvent) GetTimestamp() int64 {
+	return le.ts
+}
+
+func (le *LogEvent) GetMessage() WeakString {
+	return le.msg
+}
+
+func (le *LogEvent) GetTagLine() TagLine {
+	return TagLine(le.tgl.String())
+}
+
+func (le *LogEvent) GetTGroupId() int64 {
+	return le.tgid
+}
+
+func (le *LogEvent) SetTGroupId(id int64) {
+	le.tgid = id
+}
+
+// BufSize returns size of marshalled data
+func (le *LogEvent) BufSize() int {
+	if len(le.tgl) == 0 {
+		// tgid(8bts)+ ts(8bts) + msgLen(4 bts) + msg
+		return 20 + len(le.msg)
+	}
+	// tgid(8bts)+ ts(8bts) + msgLen(4 bts) + msg + tglLen(4 bts) + tgl
+	return 24 + len(le.msg) + len(le.tgl)
+}
+
+// MarshalTagGroupIdOnly marshals tagId from the le to provided buffer supposing
+// that the buf is marshalled event
+func (le *LogEvent) MarshalTagGroupIdOnly(buf []byte) (int, error) {
+	return MarshalInt64(int64(le.tgid), buf)
+}
+
+func (le *LogEvent) Marshal(buf []byte) (int, error) {
+	n, err := MarshalInt64(int64(le.tgid), buf)
+	if err != nil {
+		return 0, err
+	}
+
+	n1, err := MarshalInt64(le.ts, buf[n:])
+	if err != nil {
+		return 0, err
+	}
+	n += n1
+
+	n1, err = MarshalString(string(le.msg), buf[n:])
+	if err != nil {
+		return 0, err
+	}
+
+	if len(le.tgl) > 0 {
+		n += n1
+		n1, err = MarshalString(string(le.tgl), buf[n:])
+	} else {
+		buf[n] |= byte(128)
+	}
+
+	return n + n1, err
+}
+
+func (le *LogEvent) Unmarshal(buf []byte) (int, error) {
+	n, tgid, err := UnmarshalInt64(buf)
+	if err != nil {
+		return 0, err
+	}
+	le.tgid = tgid
+
+	var n1 int
+	n1, le.ts, err = UnmarshalInt64(buf[n:])
+	if err != nil {
+		return 0, err
+	}
+	n += n1
+
+	lb := buf[n]
+	buf[n] &= byte(127)
+
+	n1, le.msg, err = UnmarshalString(buf[n:])
+	if err != nil {
+		return 0, err
+	}
+	buf[n] = lb
+	n += n1
+
+	if lb&128 == 0 {
+		n1, le.tgl, err = UnmarshalString(buf[n:])
+		n += n1
+	}
+
+	return n, err
+}
+
+func (le *LogEvent) String() string {
+	return fmt.Sprint("{tGroupId:", le.tgid, ", ts:", uint64(le.ts), ", msg:", le.msg, ", tgl:", le.tgl, "}")
+}
diff --git a/pkg/logevent/log_event_test.go b/pkg/logevent/log_event_test.go
new file mode 100644
index 0000000..fd28a3b
--- /dev/null
+++ b/pkg/logevent/log_event_test.go
@@ -0,0 +1,80 @@
+package logevent
+
+import (
+	"testing"
+)
+
+func BenchmarkMarshalLogEvent(b *testing.B) {
+	var le LogEvent
+	le.Init(123456, tstStr)
+	var store [200]byte
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		le.Marshal(store[:])
+	}
+}
+
+func BenchmarkUnmarshalLogEventFast(b *testing.B) {
+	var le LogEvent
+	le.InitWithTagLine(123456, tstStr, tstTags)
+	var store [2000]byte
+	le.Marshal(store[:])
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		le.Unmarshal(store[:])
+	}
+}
+
+func TestLogEventMarshalUnmarshal(t *testing.T) {
+	var le LogEvent
+	le.Init(123412341234123, "ha ha ha")
+	leMarshalUnmarshal(t, &le)
+
+	le.InitWithTagLine(12313, "abc", TagLine("abcd"))
+	leMarshalUnmarshal(t, &le)
+
+	le.tgid = 12349999999
+	leMarshalUnmarshal(t, &le)
+}
+
+func leMarshalUnmarshal(t *testing.T, le *LogEvent) {
+	bf := make([]byte, le.BufSize())
+	n, err := le.Marshal(bf)
+	if n != len(bf) || err != nil {
+		t.Fatal("Expecting n=", n, " == ", len(bf), ", err=", err)
+	}
+
+	var le2 LogEvent
+	n1, err := le2.Unmarshal(bf)
+	if n != n1 || err != nil {
+		t.Fatal("Expecting n1=", n, ", but it is ", n1, " or err is not nil: err=", err)
+	}
+
+	if le.GetMessage() != le2.GetMessage() || le.GetTimestamp() != le2.GetTimestamp() ||
+		le.GetTagLine() != le2.GetTagLine() || le.GetTGroupId() != le2.GetTGroupId() {
+		t.Fatal("Expecting le2=", le, ", but le2=", &le2)
+	}
+}
+
+func TestMarshalUnmarshalTGIOnly(t *testing.T) {
+	var le LogEvent
+	le.Init(1234, WeakString("Hello"))
+	bf := make([]byte, le.BufSize())
+	n, err := le.Marshal(bf)
+	if n != len(bf) || err != nil {
+		t.Fatal("Expecting n=", n, " == ", len(bf), ", err=", err)
+	}
+
+	var le2 LogEvent
+	le2.SetTGroupId(333)
+	le2.MarshalTagGroupIdOnly(bf)
+	n1, err := le2.Unmarshal(bf)
+	if n != n1 || err != nil {
+		t.Fatal("Expecting n1=", n, ", but it is ", n1, " or err is not nil: err=", err)
+	}
+
+	if le.GetMessage() != le2.GetMessage() || le.GetTimestamp() != le2.GetTimestamp() ||
+		le.GetTagLine() != le2.GetTagLine() || le.GetTGroupId() == le2.GetTGroupId() || le.GetTGroupId() != 0 || le2.GetTGroupId() != 333 {
+		t.Fatal("Expecting le2=", le, ", but le2=", &le2)
+	}
+}
diff --git a/pkg/logevent/tags.go b/pkg/logevent/tags.go
new file mode 100644
index 0000000..7b15dbb
--- /dev/null
+++ b/pkg/logevent/tags.go
@@ -0,0 +1,143 @@
+package logevent
+
+import (
+	"bytes"
+	"encoding/json"
+	"fmt"
+	"sort"
+	"strings"
+)
+
+type (
+	// TagLine contains a list of tags in a form |tag1=val1|tag2=val2|...| the tags
+	// are sorted alphabetically in ascending order
+	TagLine string
+
+	// TagMap is immutable storage where the key is the tag name and it is holded by its value
+	TagMap map[string]string
+
+	// An immutable structure which holds a reference to the TagMap
+	Tags struct {
+		gId int64
+		tl  TagLine
+		tm  TagMap
+	}
+
+	tagsJson struct {
+		GID     int64   `json:"gid"`
+		TagLine TagLine `json:"tagLine"`
+	}
+)
+
+const (
+	cTagValueSeparator = "="
+	cTagSeparator      = "|"
+
+	TAG_TIMESTAMP = "ts"
+	TAG_MESSAGE   = "msg"
+	TAG_JOURNAL   = "src"
+)
+
+var (
+	EmptyTagMap = TagMap(map[string]string{})
+)
+
+func (tl *TagLine) NewTags(id int64) (Tags, error) {
+	if *tl == "" {
+		return Tags{gId: id, tl: *tl, tm: EmptyTagMap}, nil
+	}
+	m, err := tl.newTagMap()
+	if err != nil {
+		return Tags{}, err
+	}
+
+	return Tags{gId: id, tl: m.BuildTagLine(), tm: m}, nil
+}
+
+func (tl *TagLine) newTagMap() (TagMap, error) {
+	vals := strings.Split(string(*tl), cTagSeparator)
+	m := make(TagMap, len(vals))
+	for _, v := range vals {
+		kv := strings.Split(v, cTagValueSeparator)
+		if len(kv) != 2 {
+			return m, fmt.Errorf("Wrong tag format: %s expecting in a form key=value", v)
+		}
+		m[kv[0]] = kv[1]
+	}
+	return m, nil
+}
+
+func NewTagMap(m map[string]string) (TagMap, error) {
+	tm := make(TagMap, len(m))
+	for k, v := range m {
+		key := strings.ToLower(k)
+		if _, ok := tm[key]; ok {
+			return nil, fmt.Errorf("Incorrect tag initializing map, expecting keys to be case insensitive, but it is %v", m)
+		}
+		tm[key] = v
+	}
+	return tm, nil
+}
+
+func (tm *TagMap) NewTags(id int64) (Tags, error) {
+	return Tags{gId: id, tl: tm.BuildTagLine(), tm: *tm}, nil
+}
+
+// BuildTagLine builds the TagLine from the map of values
+func (tm *TagMap) BuildTagLine() TagLine {
+	srtKeys := make([]string, 0, len(*tm))
+	// sort keys
+	for k := range *tm {
+		idx := sort.SearchStrings(srtKeys, k)
+		srtKeys = append(srtKeys, k)
+		if idx < len(srtKeys)-1 {
+			copy(srtKeys[idx+1:], srtKeys[idx:])
+		}
+		srtKeys[idx] = k
+	}
+
+	var b bytes.Buffer
+	first := true
+	for _, k := range srtKeys {
+		if !first {
+			b.WriteString(cTagSeparator)
+		}
+		b.WriteString(k)
+		b.WriteString(cTagValueSeparator)
+		b.WriteString((*tm)[k])
+		first = false
+	}
+	return TagLine(b.String())
+}
+
+func (tags *Tags) GetId() int64 {
+	return tags.gId
+}
+
+func (tags *Tags) GetTagLine() TagLine {
+	return tags.tl
+}
+
+func (tags *Tags) GetValue(key string) string {
+	return tags.tm[key]
+}
+
+func (tags *Tags) MarshalJSON() ([]byte, error) {
+	return json.Marshal(&tagsJson{tags.gId, tags.tl})
+}
+
+func (tags *Tags) UnmarshalJSON(data []byte) error {
+	var res tagsJson
+	err := json.Unmarshal(data, &res)
+	if err != nil {
+		return err
+	}
+	tags.gId = res.GID
+	tags.tl = TagLine(res.TagLine)
+	tags.tm, err = tags.tl.newTagMap()
+	return err
+}
+
+func (tags *Tags) String() string {
+	return fmt.Sprintf("{tid=%d, tl=%s}", tags.gId, tags.tl)
+}
diff --git a/pkg/logevent/tags_test.go b/pkg/logevent/tags_test.go
new file mode 100644
index 0000000..864402b
--- /dev/null
+++ b/pkg/logevent/tags_test.go
@@ -0,0 +1,67 @@
+package logevent
+
+import (
+	"reflect"
+	"testing"
+)
+
+func TestTLNewTags(t *testing.T) {
+	tl := TagLine("")
+	tags, err := tl.NewTags(123)
+	if err != nil || tags.gId != 123 || tags.tl != tl || len(tags.tm) != 0 {
+		t.Fatal("Expecting ok, but err=", err)
+	}
+
+	tl = TagLine("wrongvalue")
+	tags, err = tl.NewTags(123)
+	if err == nil {
+		t.Fatal("Expecting wrong value, but tags=", tags)
+	}
+
+	tl = TagLine("k=value")
+	tags, err = tl.NewTags(12)
+	if err != nil || tags.gId != 12 || tags.tl != tl || len(tags.tm) != 1 || tags.tm["k"] != "value" {
+		t.Fatal("Expecting ok, but err=", err, " ", tags)
+	}
+}
+
+func TestTMNewTagMap(t *testing.T) {
+	tm := TagMap{}
+	tags, err := tm.NewTags(123)
+	if tags.gId != 123 || tags.tl != "" || len(tags.tm) != 0 {
+		t.Fatal("Expecting ok, but err=", err, " tags=", tags)
+	}
+
+	tm = TagMap{"c": "aaa", "a": "cccc"}
+	tags, err = tm.NewTags(34)
+	if tags.gId != 34 || tags.tl != TagLine("a=cccc|c=aaa") || !reflect.DeepEqual(tags.tm, tm) {
+		t.Fatal("Expecting ok, but err=", err, " tags=", tags)
+	}
+}
+
+func TestMarshalUnmarshalTags(t *testing.T) {
+	tl := TagLine("k=value|k1=value2")
+	tags, err := tl.NewTags(123)
+	if err != nil {
+		t.Fatal("could not create tags err=", err)
+	}
+
+	if len(tags.tm) != 2 {
+		t.Fatal("unexpected tags=", tags)
+	}
+
+	res, err := tags.MarshalJSON()
+	if err != nil {
+		t.Fatal("could not marshal err=", err)
+	}
+
+	var tags2 Tags
+	err = tags2.UnmarshalJSON(res)
+	if err != nil {
+		t.Fatal("could not unmarshal err=", err)
+	}
+
+	if !reflect.DeepEqual(tags, tags2) {
+		t.Fatal("Expected ", tags, ", but got ", tags2)
+	}
+}
diff --git a/pkg/logevent/transform.go b/pkg/logevent/transform.go
new file mode 100644
index 0000000..f1d0aaa
--- /dev/null
+++ b/pkg/logevent/transform.go
@@ -0,0 +1,195 @@
+package logevent
+
+import (
+	"encoding/binary"
+	"errors"
+	"fmt"
+	"reflect"
+	"unsafe"
+)
+
+type (
+	SSlice []WeakString
+
+	// WeakString is a string is a sting which probably points to a byte slice,
+	// which context can be changed. The WeakString can be easily created without
+	// memory allocation, but has to be used with an extra care, must not be
+	// stored in a long-live collections or passed through a channel etc.
+	WeakString string
+)
+
+// String turns the WeakString to it's safe immutable version. Just copy context
+func (ws WeakString) String() string {
+	return string(StringToByteArray(string(ws)))
+}
+
+func StrSliceToSSlice(ss []string) SSlice {
+	return *(*SSlice)(unsafe.Pointer(&ss))
+}
+
+// Size() returns how much memory serialization needs
+func (ss SSlice) Size() int {
+	sz := 2
+	for _, s := range ss {
+		sz += 4 + len(s)
+	}
+	return sz
+}
+
+func MarshalSSlice(ss SSlice, buf []byte) (int, error) {
+	idx, err := MarshalUint16(uint16(len(ss)), buf)
+	if err != nil {
+		return 0, err
+	}
+
+	for _, s := range ss {
+		n, err := MarshalString(string(s), buf[idx:])
+		if err != nil {
+			return 0, err
+		}
+		idx += n
+	}
+	return idx, nil
+}
+
+func UnmarshalSSlice(ss SSlice, buf []byte) (SSlice, int, error) {
+	idx, usz, err := UnmarshalUint16(buf)
+	if err != nil {
+		return nil, 0, err
+	}
+
+	sz := int(usz)
+	if sz > cap(ss) {
+		return nil, 0, errors.New(fmt.Sprintf("Not enough space in the result slice, required capacity is %d, but actual one is %d", sz, cap(ss)))
+	}
+
+	ss = ss[:sz]
+	for i := 0; i < sz; i++ {
+		n, s, err := UnmarshalString(buf[idx:])
+		if err != nil {
+			return nil, 0, err
+		}
+		idx += n
+		ss[i] = s
+	}
+	return ss, idx, nil
+}
+
+func MarshalByte(v byte, buf []byte) (int, error) {
+	if len(buf) < 1 {
+		return 0, noBufErr("MarshalByte", len(buf), 1)
+	}
+	buf[0] = v
+	return 1, nil
+}
+
+func UnmarshalByte(buf []byte) (int, byte, error) {
+	if len(buf) == 0 {
+		return 0, 0, noBufErr("UnmarshalByte", len(buf), 1)
+	}
+	return 1, buf[0], nil
+}
+
+func MarshalUint16(v uint16, buf []byte) (int, error) {
+	if len(buf) < 2 {
+		return 0, noBufErr("MarshalUint16", len(buf), 2)
+	}
+	binary.BigEndian.PutUint16(buf, v)
+	return 2, nil
+}
+
+func UnmarshalUint16(buf []byte) (int, uint16, error) {
+	if len(buf) < 2 {
+		return 0, 0, noBufErr("UnmarshalUint16", len(buf), 2)
+	}
+	return 2, binary.BigEndian.Uint16(buf), nil
+}
+
+func MarshalUint32(v uint32, buf []byte) (int, error) {
+	if len(buf) < 4 {
+		return 0, noBufErr("MarshalUint32", len(buf), 4)
+	}
+	binary.BigEndian.PutUint32(buf, v)
+	return 4, nil
+}
+
+func UnmarshalUint32(buf []byte) (int, uint32, error) {
+	if len(buf) < 4 {
+		return 0, 0, noBufErr("UnmarshalUint32", len(buf), 4)
+	}
+	return 4, binary.BigEndian.Uint32(buf), nil
+}
+
+func MarshalInt64(v int64, buf []byte) (int, error) {
+	if len(buf) < 8 {
+		return 0, noBufErr("MarshalInt64", len(buf), 8)
+	}
+	binary.BigEndian.PutUint64(buf, uint64(v))
+	return 8, nil
+}
+
+func UnmarshalInt64(buf []byte) (int, int64, error) {
+	if len(buf) < 8 {
+		return 0, 0, noBufErr("UnmarshalInt64", len(buf), 8)
+	}
+	return 8, int64(binary.BigEndian.Uint64(buf)), nil
+}
+
+func MarshalString(v string, buf []byte) (int, error) {
+	bl := len(buf)
+	ln := len(v)
+	if ln+4 > bl {
+		return 0, noBufErr("MarshalString-size-body", bl, ln+4)
+	}
+	binary.BigEndian.PutUint32(buf, uint32(ln))
+	var src = buf[4 : ln+4]
+	dst := src
+	src = *(*[]byte)(unsafe.Pointer(&v))
+	copy(dst, src)
+	return ln + 4, nil
+}
+
+func StringToByteArray(v string) []byte {
+	var slcHdr reflect.SliceHeader
+	sh := *(*reflect.StringHeader)(unsafe.Pointer(&v))
+	slcHdr.Data = sh.Data
+	slcHdr.Cap = sh.Len
+	slcHdr.Len = sh.Len
+	return *(*[]byte)(unsafe.Pointer(&slcHdr))
+}
+
+func ByteArrayToString(buf []byte) WeakString {
+	return *(*WeakString)(unsafe.Pointer(&buf))
+}
+
+// UnmarshalString fastest, but not completely safe version of unmarshalling
+// the byte buffer to string. Please use with care and keep in mind that buf must not
+// be updated so as it will affect the string context then.
+func UnmarshalString(buf []byte) (int, WeakString, error) {
+	if len(buf) < 4 {
+		return 0, "", noBufErr("UnmarshalString-size", len(buf), 4)
+	}
+	ln := int(binary.BigEndian.Uint32(buf))
+	if ln+4 > len(buf) {
+		return 0, "", noBufErr("UnmarshalString-body", len(buf), ln+4)
+	}
+	bs := buf[4 : ln+4]
+	res := *(*string)(unsafe.Pointer(&bs))
+	return ln + 4, WeakString(res), nil
+}
+
+func MarshalStringBuf(v string, buf []byte) (int, error) {
+	if len(v) > len(buf) {
+		return 0, fmt.Errorf("could not MarshalStringBuf() - not enough space. Required %d bytes, but the buffer sized is %d", len(v), len(buf))
+	}
+	ba := StringToByteArray(v)
+	return copy(buf, ba), nil
+}
+
+func UnmarshalStringBuf(buf []byte) WeakString {
+	return ByteArrayToString(buf)
+}
+
+func noBufErr(src string, ln, req int) error {
+	return fmt.Errorf("not enough space in the buf: %s requres %d bytes, but actual buf size is %d", src, req, ln)
+}
diff --git a/pkg/logevent/transform_test.go b/pkg/logevent/transform_test.go
new file mode 100644
index 0000000..54e08ed
--- /dev/null
+++ b/pkg/logevent/transform_test.go
@@ -0,0 +1,170 @@
+package logevent
+
+import (
+	"reflect"
+	"testing"
+)
+
+var tstStr = WeakString("This is some string for test marshalling speed Yahhoooo 11111111111111111111111111111111111111111111111111")
+var tstTags = TagLine("pod=1234134kjhakfdjhlakjdsfhkjahdlf,key=abc,")
+
+func BenchmarkMarshalString(b *testing.B) {
+	var buf [200]byte
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		MarshalString(string(tstStr), buf[:])
+	}
+}
+
+func BenchmarkUnmarshalString(b *testing.B) {
+	var buf [200]byte
+	MarshalString(string(tstStr), buf[:])
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		UnmarshalString(buf[:])
+	}
+}
+
+func BenchmarkWeakStringCast(b *testing.B) {
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		s := string(tstStr)
+		tstStr = WeakString(s)
+	}
+}
+
+func BenchmarkWeakStringCastByCopy(b *testing.B) {
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		s := tstStr.String()
+		tstStr = WeakString(s)
+	}
+}
+
+func TestStrSliceToSSlice(t *testing.T) {
+	ss := []string{"a", "b", "c"}
+	s := StrSliceToSSlice(ss)
+	if len(s) != 3 || s[1] != "b" {
+		t.Fatal("cannot cast s=", s)
+	}
+
+	if reflect.TypeOf(s[0]).String() != "model.WeakString" {
+		t.Fatal("oops, some wrong underlying type ", reflect.TypeOf(s[0]))
+	}
+}
+
+func TestMarshalString(t *testing.T) {
+	str := "hello str"
+	buf := make([]byte, len(str)+4)
+	n, err := MarshalString(str, buf)
+	if err != nil {
+		t.Fatal("Should be enough space, but err=", err)
+	}
+	if n != len(str)+4 {
+		t.Fatal("expected string marshal size is ", len(str)+4, ", but actual is ", n)
+	}
+}
+
+func TestMarshalUnmarshal(t *testing.T) {
+	str := "hello str"
+	buf := make([]byte, len(str)+4)
+	MarshalString(str, buf)
+	_, str2, _ := UnmarshalString(buf)
+	if string(str2) != str {
+		t.Fatal("Wrong unmarshaling str=", str, ", str2=", str2)
+	}
+
+	buf[4] = buf[5]
+	if string(str2) == str {
+		t.Fatal("They must be different now: str=", str, ", str2=", str2)
+	}
+}
+
+func TestMarshalUnmarshalSSliceEmpty(t *testing.T) {
+	ss := []WeakString{}
+	buf := make([]byte, 100)
+	n, err := MarshalSSlice(SSlice(ss), buf)
+	if n != 2 || err != nil {
+		t.Fatal("Must be able to write 2 bytes for the SSlice length n=", n, ", err=", err)
+	}
+
+	ss, n, err = UnmarshalSSlice(ss, buf)
+	if n != 2 || err != nil || len(ss) != 0 {
+		t.Fatal("Must be able to read 2 bytes for the SSlice length n=", n, ", err=", err)
+	}
+}
+
+func TestMarshalUnmarshalSSlice(t *testing.T) {
+	ss := []WeakString{"aaa", "bbb"}
+
+	if SSlice(ss).Size() != 16 {
+		t.Fatal("Expecting marshaled size 16, but really ", SSlice(ss).Size())
+	}
+
+	buf := make([]byte, 100)
+	n, err := MarshalSSlice(SSlice(ss), buf)
+	if n != 16 || err != nil {
+		t.Fatal("Must be able to write 16 bytes for the SSlice length n=", n, ", err=", err)
+	}
+
+	s := []WeakString{"", ""}
+	ss, n, err = UnmarshalSSlice(SSlice(s[:1]), buf)
+	if n != 16 || err != nil || len(ss) != 2 || ss[0] != "aaa" || ss[1] != "bbb" {
+		t.Fatal("Must be able to read 2 bytes for the SSlice length n=", n, ", err=", err, ", ss=", ss)
+	}
+
+	s = []WeakString{""}
+	ss, n, err = UnmarshalSSlice(SSlice(s), buf)
+	if err == nil {
+		t.Fatal("Must report error - slice not big enough!")
+	}
+}
+
+func TestCasts(t *testing.T) {
+	b := make([]byte, 10)
+	s := ByteArrayToString(b)
+	b[0] = 'a'
+	if len(s) != 10 || s[0] != 'a' {
+		t.Fatal("must be pointed to same object s=", s, " b=", b)
+	}
+
+	s1 := s.String()
+	b[0] = 'b'
+	if s1[0] != 'a' || s[0] != 'b' {
+		t.Fatal("must be different objects s=", s, " b=", b, ", s1=", s1)
+	}
+
+	s = "Hello WOrld"
+	bf := StringToByteArray(string(s))
+	s2 := ByteArrayToString(bf)
+	if s != s2 {
+		t.Fatal("Oops, expecting s1=", s, ", but really s2=", s2)
+	}
+
+	bf = StringToByteArray("")
+	s2 = ByteArrayToString(bf)
+	if s2 != "" {
+		t.Fatal("Oops, expecting empty string, but got s2=", s2)
+	}
+}
+
+func TestMarshalStringBuf(t *testing.T) {
+	str := "Hello World"
+	b := []byte{}
+	n, err := MarshalStringBuf(str, b)
+	if err == nil {
+		t.Fatal("should be error, but it is not, n=", n)
+	}
+
+	b = make([]byte, 100)
+	n, err = MarshalStringBuf(str, b)
+	if err != nil || n != len(str) {
+		t.Fatal("should not be an error, but n=", n, ", err=", err)
+	}
+
+	b[0] = 'h'
+	str0 := UnmarshalStringBuf(b[:n]).String()
+	if str0 == str || str0 != "hello World" {
+		t.Fatal("Unexpected values str=", str, ", str0=", str0)
+	}
+}
diff --git a/pkg/proto/atmosphere/server.go b/pkg/proto/atmosphere/server.go
index e075fc2..110e289 100644
--- a/pkg/proto/atmosphere/server.go
+++ b/pkg/proto/atmosphere/server.go
@@ -3,6 +3,8 @@ package atmosphere
 import (
 	"context"
 	"fmt"
+	"github.com/logrange/logrange/pkg/dstruct"
+	"github.com/logrange/logrange/pkg/util"
 	"io"
 	"math"
 	"strconv"
@@ -10,8 +12,6 @@ import (
 	"time"
 
 	"github.com/jrivets/log4g"
-	"github.com/logrange/logrange/pkg/collection"
-	"github.com/logrange/logrange/pkg/util/hash"
 )
 
 type (
@@ -21,7 +21,7 @@ type (
 		scfg      ServerConfig
 		ctx       context.Context
 		ctxCancel context.CancelFunc
-		clients   *collection.Lru
+		clients   *dstruct.Lru
 		sessions  map[int]string
 		idCnt     int
 		logger    log4g.Logger
@@ -55,7 +55,7 @@ func newServer(scfg *ServerConfig) *server {
 
 	// configuring connection timeout
 	clnupTimeout := time.Duration(scfg.SessTimeoutMs) * time.Millisecond
-	s.clients = collection.NewLru(math.MaxInt64, clnupTimeout, s.onDeleteFromCache)
+	s.clients = dstruct.NewLru(math.MaxInt64, clnupTimeout, s.onDeleteFromCache)
 	if clnupTimeout <= 0 {
 		clnupTimeout = time.Duration(math.MaxInt64)
 	}
@@ -144,7 +144,7 @@ func (s *server) authenticate(sc *serverClient, ar *AuthReq) string {
 		s.logger.Warn("Rejecting ", sc, " unknown session, or authentication request ", ar)
 		return ""
 	}
-	sid = hash.NewSession(48)
+	sid = util.NewSessionId(48)
 	s.sessions[sc.id] = sid
 	return sid
 }
diff --git a/pkg/util/byte.go b/pkg/util/byte.go
new file mode 100644
index 0000000..8360b9f
--- /dev/null
+++ b/pkg/util/byte.go
@@ -0,0 +1,11 @@
+package util
+
+// BytesCopy makes a copy of src slice of bytes
+func BytesCopy(src []byte) []byte {
+	if len(src) == 0 {
+		return src
+	}
+	b := make([]byte, len(src))
+	copy(b, src)
+	return b
+}
diff --git a/pkg/util/byte_test.go b/pkg/util/byte_test.go
new file mode 100644
index 0000000..9399e17
--- /dev/null
+++ b/pkg/util/byte_test.go
@@ -0,0 +1,15 @@
+package util
+
+import (
+	"github.com/stretchr/testify/assert"
+	"testing"
+)
+
+func TestBytesCopy(t *testing.T) {
+	e := []byte{0, 1, 2, 3}
+	a := BytesCopy(e)
+
+	assert.NotNil(t, a)
+	assert.False(t, &a == &e)
+	assert.Equal(t, cap(a), cap(e))
+}
diff --git a/pkg/util/file.go b/pkg/util/file.go
new file mode 100644
index 0000000..ed84933
--- /dev/null
+++ b/pkg/util/file.go
@@ -0,0 +1,37 @@
+package util
+
+import (
+	"fmt"
+	"os"
+	"path/filepath"
+	"syscall"
+)
+
+// GetFileId generates an id by file name and its info. The id can help to identify
+// whether the file content was rewritten or not. For example, if two identifiers
+// calculated for same file name are different, we assume the file content was
+// rewritten between first and the second identifiers calculations. If the
+// identifiers are same, we assume that new data could be added to the file,
+// but previously written one stays unchanged.
+func GetFileId(file string, info os.FileInfo) string {
+	stat := info.Sys().(*syscall.Stat_t)
+	return fmt.Sprintf("%v_%v_%v", Md5(file), stat.Ino, stat.Dev)
+}
+
+// ExpandPaths walks through provided paths and turn them to list of files.
+// The input paths can, for instance, contain ["/var/log/*.log"], so the
+// method will return list of files from the /var/log/ folder, which have
+// .log extension.
+func ExpandPaths(paths []string) []string {
+	result := make([]string, 0, len(paths))
+	for _, pp := range paths {
+		gg, err := filepath.Glob(pp)
+		if err != nil {
+			continue
+		}
+		for _, g := range gg {
+			result = append(result, g)
+		}
+	}
+	return result
+}
diff --git a/pkg/util/file_test.go b/pkg/util/file_test.go
new file mode 100644
index 0000000..4ae6cde
--- /dev/null
+++ b/pkg/util/file_test.go
@@ -0,0 +1,42 @@
+package util
+
+import (
+	"github.com/stretchr/testify/assert"
+	"io/ioutil"
+	"os"
+	"strconv"
+	"strings"
+	"syscall"
+	"testing"
+)
+
+func TestGetFileId(t *testing.T) {
+	fd, err := ioutil.TempFile("/tmp", "GetFileId_")
+	if err != nil {
+		t.Fatal(err)
+	}
+
+	defer func() {
+		if fd != nil {
+			fd.Close()
+			os.Remove(fd.Name())
+		}
+	}()
+
+	fi, err := fd.Stat()
+	if err != nil {
+		t.Fatal(err)
+	}
+
+	id := GetFileId(fd.Name(), fi)
+	pp := strings.Split(id, "_")
+
+	hsh := pp[0]
+	ino, _ := strconv.ParseUint(pp[1], 10, 64)
+	dev, _ := strconv.ParseUint(pp[2], 10, 32)
+
+	assert.Equal(t, len(pp), 3)
+	assert.Equal(t, hsh, Md5(fd.Name()))
+	assert.Equal(t, ino, fi.Sys().(*syscall.Stat_t).Ino)
+	assert.Equal(t, uint64(dev), uint64(fi.Sys().(*syscall.Stat_t).Dev))
+}
diff --git a/pkg/util/format.go b/pkg/util/format.go
new file mode 100644
index 0000000..55f82f1
--- /dev/null
+++ b/pkg/util/format.go
@@ -0,0 +1,30 @@
+package util
+
+import (
+	"fmt"
+	"github.com/jrivets/gorivets"
+	"strings"
+)
+
+// FormatSize prints the size by scale 1000, ex: 23Kb(23450)
+func FormatSize(val int64) string {
+	if val < 1000 {
+		return fmt.Sprint(val)
+	}
+	return fmt.Sprint(gorivets.FormatInt64(val, 1000), "(", val, ")")
+}
+
+func FormatProgress(size int, perc float64) string {
+	fl := int(float64(size-2) * perc / 100.0)
+
+	if fl > (size - 2) {
+		fl = size - 2
+	}
+
+	if fl < 0 {
+		fl = 0
+	}
+
+	empt := size - 2 - fl
+	return fmt.Sprintf("%5.2f%% |%s%s|", perc, strings.Repeat("#", fl), strings.Repeat("-", empt))
+}
diff --git a/pkg/util/format_test.go b/pkg/util/format_test.go
new file mode 100644
index 0000000..d15554c
--- /dev/null
+++ b/pkg/util/format_test.go
@@ -0,0 +1 @@
+package util
\ No newline at end of file
diff --git a/pkg/util/hash.go b/pkg/util/hash.go
new file mode 100644
index 0000000..6eadd2c
--- /dev/null
+++ b/pkg/util/hash.go
@@ -0,0 +1,21 @@
+package util
+
+import (
+	"crypto/md5"
+	"crypto/sha256"
+	"encoding/base64"
+	"fmt"
+)
+
+func Hash(str string) string {
+	return BytesHash([]byte(str))
+}
+
+func BytesHash(bts []byte) string {
+	h := sha256.Sum256(bts)
+	return base64.StdEncoding.EncodeToString(h[:])
+}
+
+func Md5(s string) string {
+	return fmt.Sprintf("%x", md5.Sum([]byte(s)))
+}
diff --git a/pkg/util/hash/hash.go b/pkg/util/hash/hash.go
deleted file mode 100644
index c68131d..0000000
--- a/pkg/util/hash/hash.go
+++ /dev/null
@@ -1,130 +0,0 @@
-package hash
-
-import (
-	"bytes"
-	"crypto/rand"
-	"crypto/sha256"
-	"encoding/base64"
-	"fmt"
-	"net"
-	"strings"
-)
-
-const (
-	secretKeyAlphabet = "0123456789QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnazx_^-()@#$%"
-	sessionAlphabet   = "0123456789QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnazx"
-)
-
-// GetMacAddress returns a non-loopback interface MAC address. It returns an
-// error with the reason, if it is not possible to discover one.
-func GetMacAddress() ([]byte, error) {
-	addrs, err := net.InterfaceAddrs()
-	if err != nil {
-		return nil, err
-	}
-
-	var ip string
-	for _, a := range addrs {
-		if ipn, ok := a.(*net.IPNet); ok && !ipn.IP.IsLoopback() {
-			if ipn.IP.To4() != nil {
-				ip = ipn.IP.String()
-				break
-			}
-		}
-	}
-	if ip == "" {
-		return nil, fmt.Errorf("could not find any ip address except loopback")
-	}
-
-	ifss, err := net.Interfaces()
-	if err != nil {
-		return nil, err
-	}
-
-	for _, ifs := range ifss {
-		if addrs, err := ifs.Addrs(); err == nil {
-			for _, addr := range addrs {
-				if strings.Contains(addr.String(), ip) {
-					nif, err := net.InterfaceByName(ifs.Name)
-					if err != nil {
-						continue
-					}
-
-					return []byte(nif.HardwareAddr), nil
-				}
-			}
-		}
-	}
-	return nil, fmt.Errorf("could not find any interface with MAC address")
-}
-
-// Makes a session with size characters in the the string. sessionAlphabet
-// is used for making the session
-func NewSession(size int) string {
-	return GetRandomString(size, sessionAlphabet)
-}
-
-// Makes a password string
-func NewPassword(size int) string {
-	return GetRandomString(size, secretKeyAlphabet)
-}
-
-func GetRandomString(size int, abc string) string {
-	var buffer bytes.Buffer
-	var val [64]byte
-	var buf []byte
-	if size > len(val) {
-		buf = make([]byte, size)
-	} else {
-		buf = val[:size]
-	}
-	Rand(buf)
-
-	for _, v := range buf {
-		buffer.WriteString(string(abc[int(v)%len(abc)]))
-	}
-
-	return buffer.String()
-}
-
-func Hash(str string) string {
-	return BytesHash([]byte(str))
-}
-
-func BytesHash(bts []byte) string {
-	h := sha256.Sum256(bts)
-	return base64.StdEncoding.EncodeToString(h[:])
-}
-
-func Rand(bts []byte) {
-	if _, err := rand.Read(bts); err != nil {
-		panic(err)
-	}
-}
-
-// Bytes2String transforms bitstream to a string. val contains bytes and
-// only lower bits from any value is used for the calculation. abet - is an
-// alpabet which is used for forming the result string.
-func Bytes2String(val []byte, abet string, bits int) string {
-	kap := len(val) * 8 / bits
-	abl := len(abet)
-	res := make([]byte, 0, kap)
-	mask := (1 << uint(bits)) - 1
-	i := 0
-	shft := 0
-	for i < len(val) {
-		b := int(val[i]) >> uint(shft)
-		bSize := 8 - shft
-		if bSize <= bits {
-			i++
-			if i < (len(val)) {
-				shft = bits - bSize
-				b |= int(val[i]) << uint(bSize)
-			}
-		} else {
-			shft += bits
-		}
-		res = append(res, abet[(b&mask)%abl])
-	}
-	return string(res)
-}
diff --git a/pkg/util/hash/hash_test.go b/pkg/util/hash/hash_test.go
deleted file mode 100644
index 6cfbecf..0000000
--- a/pkg/util/hash/hash_test.go
+++ /dev/null
@@ -1,37 +0,0 @@
-package hash
-
-import (
-	"net"
-	"testing"
-)
-
-func TestNewSession(t *testing.T) {
-	for i := 0; i < 100; i++ {
-		s := NewSession(i)
-		if len(s) != i {
-			t.Fatal("Unexpected length ", len(s), ", but wanted ", i)
-		}
-		t.Log(s)
-	}
-}
-
-func TestNewPassword(t *testing.T) {
-	for i := 0; i < 100; i++ {
-		s := NewPassword(i)
-		if len(s) != i {
-			t.Fatal("Unexpected length ", len(s), ", but wanted ", i)
-		}
-		t.Log(s)
-	}
-}
-
-func TestGetMacAddress(t *testing.T) {
-	mac, err := GetMacAddress()
-	if err != nil {
-		t.Log("Could not find mac. err=", err)
-		return
-	}
-
-	ms, _ := net.ParseMAC(string(net.HardwareAddr(mac).String()))
-	t.Log("mac= ", ms, " ", mac)
-}
diff --git a/pkg/util/hash_test.go b/pkg/util/hash_test.go
new file mode 100644
index 0000000..c7d8682
--- /dev/null
+++ b/pkg/util/hash_test.go
@@ -0,0 +1 @@
+package util
diff --git a/pkg/util/json.go b/pkg/util/json.go
new file mode 100644
index 0000000..3dee055
--- /dev/null
+++ b/pkg/util/json.go
@@ -0,0 +1,21 @@
+package util
+
+import (
+	"bytes"
+	"encoding/json"
+	"strings"
+)
+
+// ToJsonStr encodes v to a json string. Don't use it in data streaming due to
+// bad performance and memory allocations
+func ToJsonStr(v interface{}) string {
+	buffer := &bytes.Buffer{}
+	encoder := json.NewEncoder(buffer)
+	encoder.SetEscapeHTML(false)
+	err := encoder.Encode(v)
+	if err != nil {
+		return ""
+	}
+	s := string(buffer.Bytes())
+	return strings.TrimRight(s, "\n")
+}
diff --git a/pkg/util/json_test.go b/pkg/util/json_test.go
new file mode 100644
index 0000000..c7d8682
--- /dev/null
+++ b/pkg/util/json_test.go
@@ -0,0 +1 @@
+package util
diff --git a/pkg/util/net.go b/pkg/util/net.go
new file mode 100644
index 0000000..5cafe9c
--- /dev/null
+++ b/pkg/util/net.go
@@ -0,0 +1,50 @@
+package util
+
+import (
+	"fmt"
+	"net"
+	"strings"
+)
+
+// GetMacAddress returns a non-loopback interface MAC address. It returns an
+// error with the reason, if it is not possible to discover one.
+func GetMacAddress() ([]byte, error) {
+	addrs, err := net.InterfaceAddrs()
+	if err != nil {
+		return nil, err
+	}
+
+	var ip string
+	for _, a := range addrs {
+		if ipn, ok := a.(*net.IPNet); ok && !ipn.IP.IsLoopback() {
+			if ipn.IP.To4() != nil {
+				ip = ipn.IP.String()
+				break
+			}
+		}
+	}
+	if ip == "" {
+		return nil, fmt.Errorf("could not find any ip address except loopback")
+	}
+
+	ifss, err := net.Interfaces()
+	if err != nil {
+		return nil, err
+	}
+
+	for _, ifs := range ifss {
+		if addrs, err := ifs.Addrs(); err == nil {
+			for _, addr := range addrs {
+				if strings.Contains(addr.String(), ip) {
+					nif, err := net.InterfaceByName(ifs.Name)
+					if err != nil {
+						continue
+					}
+
+					return []byte(nif.HardwareAddr), nil
+				}
+			}
+		}
+	}
+	return nil, fmt.Errorf("could not find any interface with MAC address")
+}
diff --git a/pkg/util/net_test.go b/pkg/util/net_test.go
new file mode 100644
index 0000000..b0f8d78
--- /dev/null
+++ b/pkg/util/net_test.go
@@ -0,0 +1,17 @@
+package util
+
+import (
+	"net"
+	"testing"
+)
+
+func TestGetMacAddress(t *testing.T) {
+	mac, err := GetMacAddress()
+	if err != nil {
+		t.Log("Could not find mac. err=", err)
+		return
+	}
+
+	ms, _ := net.ParseMAC(string(net.HardwareAddr(mac).String()))
+	t.Log("mac= ", ms, " ", mac)
+}
diff --git a/pkg/util/secret.go b/pkg/util/secret.go
new file mode 100644
index 0000000..2fbb5a7
--- /dev/null
+++ b/pkg/util/secret.go
@@ -0,0 +1,17 @@
+package util
+
+const (
+	secretKeyAlphabet = "0123456789QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnazx_^-()@#$%"
+	sessionAlphabet   = "0123456789QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnazx"
+)
+
+// Makes a session with size characters in the the string. sessionAlphabet
+// is used for making the session
+func NewSessionId(size int) string {
+	return GetRandomString(size, sessionAlphabet)
+}
+
+// Makes a password string
+func NewPassword(size int) string {
+	return GetRandomString(size, secretKeyAlphabet)
+}
diff --git a/pkg/util/secret_test.go b/pkg/util/secret_test.go
new file mode 100644
index 0000000..a1eb655
--- /dev/null
+++ b/pkg/util/secret_test.go
@@ -0,0 +1,23 @@
+package util
+
+import "testing"
+
+func TestNewSessionId(t *testing.T) {
+	for i := 0; i < 100; i++ {
+		s := NewSessionId(i)
+		if len(s) != i {
+			t.Fatal("Unexpected length ", len(s), ", but wanted ", i)
+		}
+		t.Log(s)
+	}
+}
+
+func TestNewPassword(t *testing.T) {
+	for i := 0; i < 100; i++ {
+		s := NewPassword(i)
+		if len(s) != i {
+			t.Fatal("Unexpected length ", len(s), ", but wanted ", i)
+		}
+		t.Log(s)
+	}
+}
diff --git a/pkg/util/string.go b/pkg/util/string.go
new file mode 100644
index 0000000..86c4e9f
--- /dev/null
+++ b/pkg/util/string.go
@@ -0,0 +1,81 @@
+package util
+
+import (
+	"bytes"
+	"crypto/rand"
+	"reflect"
+	"unsafe"
+)
+
+func GetRandomString(size int, abc string) string {
+	var buffer bytes.Buffer
+	var val [64]byte
+	var buf []byte
+
+	if size > len(val) {
+		buf = make([]byte, size)
+	} else {
+		buf = val[:size]
+	}
+
+	if _, err := rand.Read(buf); err != nil {
+		panic(err)
+	}
+
+	for _, v := range buf {
+		buffer.WriteString(string(abc[int(v)%len(abc)]))
+	}
+
+	return buffer.String()
+}
+
+// Bytes2String transforms bitstream to a string. val contains bytes and
+// only lower bits from any value is used for the calculation. abet - is an
+// alpabet which is used for forming the result string.
+func Bytes2String(val []byte, abet string, bits int) string {
+	kap := len(val) * 8 / bits
+	abl := len(abet)
+	res := make([]byte, 0, kap)
+	mask := (1 << uint(bits)) - 1
+	i := 0
+	shft := 0
+	for i < len(val) {
+		b := int(val[i]) >> uint(shft)
+		bSize := 8 - shft
+		if bSize <= bits {
+			i++
+			if i < (len(val)) {
+				shft = bits - bSize
+				b |= int(val[i]) << uint(bSize)
+			}
+		} else {
+			shft += bits
+		}
+		res = append(res, abet[(b&mask)%abl])
+	}
+	return string(res)
+}
+
+// StringToByteArray makes unsafe cast of a string to byte array
+func StringToByteArray(v string) []byte {
+	var slcHdr reflect.SliceHeader
+	sh := *(*reflect.StringHeader)(unsafe.Pointer(&v))
+	slcHdr.Data = sh.Data
+	slcHdr.Cap = sh.Len
+	slcHdr.Len = sh.Len
+	return *(*[]byte)(unsafe.Pointer(&slcHdr))
+}
+
+// RemoveDups returns a slice where every element from ss meets only once
+func RemoveDups(ss []string) []string {
+	j := 0
+	found := map[string]bool{}
+	for i, s := range ss {
+		if !found[s] {
+			found[s] = true
+			ss[j] = ss[i]
+			j++
+		}
+	}
+	return ss[:j]
+}
diff --git a/pkg/util/string_test.go b/pkg/util/string_test.go
new file mode 100644
index 0000000..75788d5
--- /dev/null
+++ b/pkg/util/string_test.go
@@ -0,0 +1,14 @@
+package util
+
+import (
+	"testing"
+	"github.com/stretchr/testify/assert"
+)
+
+func TestRemoveDups(t *testing.T) {
+	e := []string{"a", "b", "c", "d"}
+	a := RemoveDups([]string{"a", "a", "b", "c", "b", "c", "d"})
+
+	assert.NotNil(t, a)
+	assert.ElementsMatch(t, a, e)
+}
-- 
2.14.3 (Apple Git-98)

